{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import stk.random\n",
    "import stk.ops\n",
    "\n",
    "# Use standard block size of 128\n",
    "block_size = 128\n",
    "\n",
    "# Ensure dimensions are multiples of block_size\n",
    "m = 1024  # 1024 = 8 * 128 ✓\n",
    "n = 2048  # 2048 = 16 * 128 ✓ \n",
    "hidden_size = 512  # 512 = 4 * 128 ✓\n",
    "\n",
    "# Create the topology\n",
    "sparsity = 0.5\n",
    "topo = stk.random.mask(m, n, sparsity, block_size).to('cuda')\n",
    "\n",
    "# First operation: sdd (dense × dense → sparse)\n",
    "a = torch.randn(m, hidden_size, device='cuda')\n",
    "w1 = torch.randn(hidden_size, n, device='cuda')\n",
    "block_sparse = stk.ops.sdd(a, w1, topo)\n",
    "\n",
    "# Second operation: dsd (sparse × dense → dense)\n",
    "w2 = torch.randn(n, hidden_size, device='cuda')\n",
    "output = stk.ops.dsd(block_sparse, w2) #dsd outputs a tensor! very nice. \n",
    "\n",
    "print(f\"Output shape: {output.shape}\")  # Should be (m, hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "selected_experts:\n",
      " tensor([[3, 5],\n",
      "        [2, 5],\n",
      "        [7, 2],\n",
      "        ...,\n",
      "        [2, 3],\n",
      "        [0, 3],\n",
      "        [4, 2]], device='cuda:0')\n",
      "selected_experts_sorted:\n",
      " tensor([0, 0, 0,  ..., 7, 7, 7], device='cuda:0')\n",
      "router_weights_sorted:\n",
      " tensor([[0.5580],\n",
      "        [0.6590],\n",
      "        [0.6197],\n",
      "        ...,\n",
      "        [0.3672],\n",
      "        [0.5748],\n",
      "        [0.4858]], device='cuda:0', grad_fn=<IndexBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import stk.random\n",
    "import stk.ops\n",
    "from einops import rearrange\n",
    "\n",
    "# Setup\n",
    "block_size = 128\n",
    "m, n = 1024, 2048\n",
    "hidden_size = 512\n",
    "num_experts = 8 \n",
    "num_experts_per_tok = 2\n",
    "norm_topk_prob = True\n",
    "batch_size = 4\n",
    "\n",
    "router = nn.Linear(hidden_size, num_experts, bias=False,device='cuda')\n",
    "\n",
    "def route_tokens(x_flat):\n",
    "    \"\"\"Route tokens to experts and compute weights.\"\"\"\n",
    "    router_logits = router(x_flat)\n",
    "    router_weights = F.softmax(router_logits, dim=1, dtype=torch.float)\n",
    "    router_weights, selected_experts = torch.topk(router_weights, num_experts_per_tok, dim=-1)\n",
    "    \n",
    "    if norm_topk_prob:\n",
    "        router_weights /= router_weights.sum(dim=-1, keepdim=True)\n",
    "    \n",
    "    router_weights = router_weights.to(x_flat.dtype)\n",
    "    return router_weights, selected_experts, router_logits\n",
    "\n",
    "def sort_by_expert(x_flat, router_weights, selected_experts):\n",
    "    \"\"\"Replicate tokens for each expert and sort by expert assignment.\"\"\"\n",
    "    x_rep = x_flat.repeat_interleave(num_experts_per_tok, dim=0)\n",
    "    selected_experts_rep = selected_experts.reshape(-1)\n",
    "    router_weights_rep = router_weights.reshape(-1, 1)\n",
    "    \n",
    "    expert_sort_indices = torch.argsort(selected_experts_rep, stable=True)\n",
    "    x_sorted = x_rep[expert_sort_indices]\n",
    "    selected_experts_sorted = selected_experts_rep[expert_sort_indices]\n",
    "    router_weights_sorted = router_weights_rep[expert_sort_indices]\n",
    "    \n",
    "    inv_expert_sort_indices = torch.empty_like(expert_sort_indices)\n",
    "    \n",
    "    return x_sorted, selected_experts_sorted, router_weights_sorted, inv_expert_sort_indices\n",
    "\n",
    "def make_topology(): #???\n",
    "    pass\n",
    "\n",
    "\n",
    "\n",
    "# Create topology (this doesn't need gradients)\n",
    "# topo = stk.random.mask(m, n, sparsity, block_size).to('cuda')\n",
    "\n",
    "# Create input and weights WITH gradient tracking\n",
    "x = torch.randn(batch_size, m, hidden_size, device='cuda', requires_grad=True)\n",
    "w1 = torch.randn(hidden_size, n, device='cuda', requires_grad=True)\n",
    "w2 = torch.randn(n, hidden_size, device='cuda', requires_grad=True)\n",
    "\n",
    "x_flat = rearrange(x, 'batch seq hidden -> (batch seq) hidden')\n",
    "\n",
    "router_weights, selected_experts, router_logits = route_tokens(x_flat)\n",
    "\n",
    "x_sorted, selected_experts_sorted, router_weights_sorted, inv_indices = sort_by_expert(x_flat, router_weights, selected_experts)\n",
    "\n",
    "print(\"selected_experts:\\n\", selected_experts)\n",
    "print(\"selected_experts_sorted:\\n\", selected_experts_sorted)\n",
    "# topo = make_topology()#??\n",
    "\n",
    "# # Forward pass\n",
    "# sparse_hidden = stk.ops.sdd(x, w1, topo)  # x @ w1 with sparse output\n",
    "# output = stk.ops.dsd(sparse_hidden, w2)   # sparse @ w2 with dense output\n",
    "\n",
    "# # Create a simple loss\n",
    "# target = torch.randn_like(output)\n",
    "# loss = torch.nn.functional.mse_loss(output, target)\n",
    "\n",
    "# print(f\"Loss: {loss.item():.4f}\")\n",
    "\n",
    "# # Backward pass\n",
    "# loss.backward()\n",
    "\n",
    "# # Check gradients\n",
    "# print(f\"\\nGradients computed:\")\n",
    "# print(f\"x.grad shape: {x.grad.shape}, norm: {x.grad.norm().item():.4f}\")\n",
    "# print(f\"w1.grad shape: {w1.grad.shape}, norm: {w1.grad.norm().item():.4f}\")\n",
    "# print(f\"w2.grad shape: {w2.grad.shape}, norm: {w2.grad.norm().item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([   0,    1,    2,  ..., 8189, 8190, 8191], device='cuda:0')\n",
      "tensor([1152, 1024, 1024, 1152, 1024, 1024, 1024, 1152], device='cuda:0')\n",
      "Tokens per expert: tensor([1069, 1011,  995, 1034, 1021, 1021, 1000, 1041], device='cuda:0')\n",
      "Padded blocks per expert: tensor([9, 8, 8, 9, 8, 8, 8, 9], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import stk.ops\n",
    "import stk.matrix\n",
    "from einops import rearrange\n",
    "\n",
    "# Setup\n",
    "block_size = 128\n",
    "m, n = 1024, 2048\n",
    "hidden_size = 512\n",
    "num_experts = 8 \n",
    "num_experts_per_tok = 2\n",
    "expert_capacity = hidden_size  # Each expert has hidden_size dimension\n",
    "norm_topk_prob = True\n",
    "batch_size = 4\n",
    "\n",
    "router = nn.Linear(hidden_size, num_experts, bias=False, device='cuda')\n",
    "\n",
    "def route_tokens(x_flat):\n",
    "    \"\"\"Route tokens to experts and compute weights.\"\"\"\n",
    "    router_logits = router(x_flat)\n",
    "    router_weights = F.softmax(router_logits, dim=1, dtype=torch.float)\n",
    "    router_weights, selected_experts = torch.topk(router_weights, num_experts_per_tok, dim=-1)\n",
    "    \n",
    "    if norm_topk_prob:\n",
    "        router_weights /= router_weights.sum(dim=-1, keepdim=True)\n",
    "    \n",
    "    router_weights = router_weights.to(x_flat.dtype)\n",
    "    return router_weights, selected_experts, router_logits\n",
    "\n",
    "def sort_by_expert(x_flat, router_weights, selected_experts):\n",
    "    \"\"\"Replicate tokens for each expert and sort by expert assignment.\"\"\"\n",
    "    x_rep = x_flat.repeat_interleave(num_experts_per_tok, dim=0)\n",
    "    selected_experts_rep = selected_experts.reshape(-1)\n",
    "    router_weights_rep = router_weights.reshape(-1, 1)\n",
    "    \n",
    "    expert_sort_indices = torch.argsort(selected_experts_rep, stable=True)\n",
    "    x_sorted = x_rep[expert_sort_indices]\n",
    "    selected_experts_sorted = selected_experts_rep[expert_sort_indices]\n",
    "    router_weights_sorted = router_weights_rep[expert_sort_indices]\n",
    "    \n",
    "    # Compute inverse indices for unsort\n",
    "    inv_expert_sort_indices = torch.empty_like(expert_sort_indices)\n",
    "    inv_expert_sort_indices[expert_sort_indices] = torch.arange(\n",
    "        len(expert_sort_indices), device=expert_sort_indices.device\n",
    "    )\n",
    "    \n",
    "    return x_sorted, selected_experts_sorted, router_weights_sorted, inv_expert_sort_indices\n",
    "\n",
    "\n",
    "def pad_to_blocks( x_sorted, selected_experts_sorted):\n",
    "    \"\"\"Pad each expert's tokens to multiples of block_size and track unpadding indices.\"\"\"\n",
    "    device = x_sorted.device\n",
    "    num_tokens = x_sorted.shape[0]\n",
    "    token_dim = x_sorted.shape[-1]\n",
    "\n",
    "    # Use self.num_experts and self.block_size directly\n",
    "    min_tokens_or_experts = min(num_tokens, num_experts)\n",
    "    max_blocks = min_tokens_or_experts + (num_tokens - min_tokens_or_experts) // block_size\n",
    "    capacity_tokens = max_blocks * block_size\n",
    "\n",
    "    # Per-expert counts via scatter_add (compile-safe; avoids bincount)\n",
    "    tokens_per_expert = torch.zeros(num_experts, dtype=torch.long, device=device)\n",
    "    ones = torch.ones_like(selected_experts_sorted, dtype=torch.long)\n",
    "    tokens_per_expert.scatter_add_(0, selected_experts_sorted, ones)\n",
    "\n",
    "    # Round each expert up to a multiple of block_size\n",
    "    tokens_per_expert_padded = ((tokens_per_expert + block_size - 1) // block_size) * block_size\n",
    "\n",
    "    # Exclusive-prefix sums (orig vs padded) for placement\n",
    "    offset_original = F.pad(tokens_per_expert.cumsum(0), (1, 0))\n",
    "    offset_padded  = F.pad(tokens_per_expert_padded.cumsum(0), (1, 0))\n",
    "\n",
    "    # Allocate fixed capacity once; the tail is never indexed\n",
    "    x_padded = x_sorted.new_zeros((capacity_tokens, token_dim))\n",
    "\n",
    "    \n",
    "    # Map each sorted token to its padded position\n",
    "    token_idx = torch.arange(num_tokens, device=x_sorted.device)\n",
    "    print(token_idx)\n",
    "    idx_within_expert = token_idx - offset_original[selected_experts_sorted]\n",
    "    print(tokens_per_expert_padded)\n",
    "    unpad_indices = idx_within_expert + offset_padded[selected_experts_sorted]\n",
    "\n",
    "    # Scatter the actual tokens into their padded slots\n",
    "    x_padded[unpad_indices] = x_sorted\n",
    "\n",
    "    # Return exactly what you wanted\n",
    "    return x_padded, tokens_per_expert_padded, unpad_indices\n",
    "\n",
    "\n",
    "def create_sparse_indices(tokens_per_expert_padded):\n",
    "    device = tokens_per_expert_padded.device\n",
    "\n",
    "    # Compute blocks per expert (vectorized)\n",
    "    num_token_blocks_per_expert = tokens_per_expert_padded // block_size\n",
    "    blocks_per_expert = num_token_blocks_per_expert * _num_ffn_blocks\n",
    "\n",
    "    # Convert to Python int ONCE to avoid dynamic shapes in torch.compile\n",
    "    # This single .item() call prevents thousands of shape guard checks\n",
    "    total_blocks = int(blocks_per_expert.sum())\n",
    "\n",
    "    indices = torch.arange(total_blocks, device=device, dtype=torch.long)\n",
    "\n",
    "    # Single cumsum for expert assignment\n",
    "    cumsum = blocks_per_expert.cumsum(0)\n",
    "    expert_ids = torch.searchsorted(cumsum, indices, right=True).clamp(max=self.num_experts - 1)\n",
    "\n",
    "    # Fuse within-expert index computation\n",
    "    cumsum_padded = F.pad(cumsum[:-1], (1, 0))\n",
    "    within_expert_idx = indices - cumsum_padded[expert_ids]\n",
    "\n",
    "    # Compute row indices (combine operations)\n",
    "    token_block_cumsum = num_token_blocks_per_expert.cumsum(0)\n",
    "    token_block_offset = F.pad(token_block_cumsum[:-1], (1, 0))\n",
    "\n",
    "    # Fast modulo and division using bit operations when possible\n",
    "    # Since _num_ffn_blocks is often a power of 2, we can optimize\n",
    "    within_expert_block = within_expert_idx // self._num_ffn_blocks\n",
    "    within_expert_ffn = within_expert_idx % self._num_ffn_blocks\n",
    "\n",
    "    row_indices = token_block_offset[expert_ids] + within_expert_block\n",
    "    weight_col_indices = expert_ids * self._num_ffn_blocks + within_expert_ffn\n",
    "    output_col_indices = within_expert_ffn\n",
    "\n",
    "    return row_indices.int(), weight_col_indices.int(), output_col_indices.int()\n",
    "\n",
    "# Your existing code\n",
    "x = torch.randn(batch_size, m, hidden_size, device='cuda', requires_grad=True)\n",
    "x_flat = rearrange(x, 'batch seq hidden -> (batch seq) hidden')\n",
    "\n",
    "router_weights, selected_experts, router_logits = route_tokens(x_flat)\n",
    "x_sorted, selected_experts_sorted, router_weights_sorted, inv_indices = sort_by_expert(\n",
    "    x_flat, router_weights, selected_experts\n",
    ")\n",
    "\n",
    "x_padded, tokens_per_expert_padded, unpad_indices = pad_to_blocks(x_sorted, selected_experts_sorted)\n",
    "\n",
    "\n",
    "# Create the topology\n",
    "# topology, padded_tokens_per_expert = make_topology(\n",
    "\n",
    "\n",
    "# print(f\"Topology shape: {topology.size()}\")\n",
    "print(f\"Tokens per expert: {torch.bincount(selected_experts_sorted, minlength=num_experts)}\")\n",
    "print(f\"Padded blocks per expert: {tokens_per_expert_padded//128}\")\n",
    "\n",
    "# # Now you can use the topology for sparse matmul\n",
    "# # Create expert weights (all experts stacked)\n",
    "# all_expert_weights = torch.randn(\n",
    "#     num_experts * expert_capacity, \n",
    "#     n, \n",
    "#     device='cuda',\n",
    "#     requires_grad=True\n",
    "# )\n",
    "\n",
    "# # Pad x_sorted to match topology expectations\n",
    "# total_padded = padded_tokens_per_expert.sum().item()\n",
    "# if x_sorted.shape[0] < total_padded:\n",
    "#     padding = total_padded - x_sorted.shape[0]\n",
    "#     x_padded = torch.cat([\n",
    "#         x_sorted,\n",
    "#         torch.zeros(padding, hidden_size, device='cuda')\n",
    "#     ])\n",
    "# else:\n",
    "#     x_padded = x_sorted\n",
    "# print(x_padded.shape)\n",
    "# print(all_expert_weights.shape)\n",
    "# # Sparse matrix multiply with expert weights\n",
    "# w1 = torch.randn(hidden_size, n, device='cuda', requires_grad=True)\n",
    "# w2 = torch.randn(n, hidden_size, device='cuda', requires_grad=True)\n",
    "\n",
    "# result_sparse = stk.ops.sdd(x_padded, w1, topology)\n",
    "# result = stk.ops.to_dense(result_sparse)\n",
    "\n",
    "# print(f\"Result shape: {result.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
