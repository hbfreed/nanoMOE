{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from einops import rearrange\n",
    "\n",
    "import stk\n",
    "import stk.ops\n",
    "import stk.random\n",
    "import stk.matrix\n",
    "from megablocks.layers.gelu import gelu\n",
    "\n",
    "from model import GPT, GPTConfig, MoeMLP\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = GPTConfig(\n",
    "    n_layer = 12,\n",
    "    n_head = 12,\n",
    "    n_embd = 768,\n",
    "    bias = False,\n",
    "    vocab_size=50304,\n",
    "\n",
    "    # MoE configuration with VARIABLE-SIZE EXPERTS\n",
    "    use_moe = True,\n",
    "    num_experts = 8,\n",
    "    num_experts_per_tok = 2,\n",
    "    norm_topk_prob = True,\n",
    "    block_size = 128,\n",
    "    block_k = 64,\n",
    "    expert_sizes = [(4, 2944), (4, 128)]  # 4 large (2944) + 4 small (128)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Subclass the MoE MLP layer and GPT layer to track token routing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MoeMLPWithTracking(MoeMLP):\n",
    "    \"\"\"Add expert assignment tracking to the mlp layer's forward pass\"\"\"\n",
    "\n",
    "    @torch.compiler.disable\n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, n_embd = x.shape\n",
    "\n",
    "        x_flat = rearrange(x, 'batch_size seq_len n_embd -> (batch_size seq_len) n_embd')\n",
    "\n",
    "        router_logits = self.router(x_flat)\n",
    "        router_probs = F.softmax(router_logits, dim=-1, dtype=torch.float32)\n",
    "        expert_weights, selected_experts = torch.topk(router_probs, self.num_experts_per_tok, dim=-1)\n",
    "\n",
    "        if self.norm_topk_prob:\n",
    "            expert_weights = expert_weights / expert_weights.sum(dim=-1, keepdim=True)\n",
    "        expert_weights = expert_weights.to(x.dtype)\n",
    "        expert_weights_flat = rearrange(expert_weights, '... -> (...)')\n",
    "        selected_experts_flat = rearrange(selected_experts, '... -> (...)')\n",
    "\n",
    "        bin_ids, indices, tokens_per_expert = self._sort_tokens_by_expert(selected_experts_flat)\n",
    "        padded_bins, topology = self._create_topology(x_flat, tokens_per_expert)\n",
    "        x_permuted = self._gather_tokens(x_flat, indices, bin_ids, tokens_per_expert, padded_bins)\n",
    "        x_permuted = stk.ops.sdd(x_permuted, self.w1, topology)\n",
    "        x_permuted = gelu(x_permuted)\n",
    "        x_permuted = stk.ops.dsd(x_permuted, self.w2)\n",
    "\n",
    "        x_permuted = self._scatter_tokens(x_permuted, indices, bin_ids, expert_weights_flat, tokens_per_expert, padded_bins)\n",
    "        output = rearrange(x_permuted, '(batch_size seq_len) n_embd -> batch_size seq_len n_embd')\n",
    "\n",
    "        router_z_loss = torch.logsumexp(router_logits, dim=-1).pow(2).mean()\n",
    "\n",
    "        p_i = router_probs.mean(dim=0).to(torch.bfloat16) #cast back to bfloat16 to be able to dot product with f_i\n",
    "\n",
    "        experts_flat = selected_experts.flatten()\n",
    "        f_i = torch.zeros(self.num_experts, dtype=x.dtype, device=x.device)\n",
    "        ones = torch.ones_like(experts_flat, dtype=x.dtype) / len(experts_flat)\n",
    "        f_i.scatter_add(0, experts_flat, ones)\n",
    "        load_balance_loss = self.num_experts * (f_i @ p_i)\n",
    "        \n",
    "        expert_assignments = rearrange(selected_experts, '(batch seq) k -> batch seq k', batch=batch_size, seq=seq_len)\n",
    "        router_logits_reshaped = rearrange(router_logits, '(batch seq) num_experts -> batch seq num_experts', batch=batch_size, seq=seq_len)\n",
    "        router_probs_reshaped = rearrange(router_probs, '(batch seq) num_experts -> batch seq num_experts', batch=batch_size, seq=seq_len)\n",
    "\n",
    "        aux_loss = {\n",
    "            'router_z_loss': router_z_loss,\n",
    "            'load_balance_loss': load_balance_loss,\n",
    "            'expert_assignments': expert_assignments,\n",
    "            'router_logits': router_logits_reshaped,\n",
    "            'router_probs': router_probs_reshaped,\n",
    "        }\n",
    "        \n",
    "        return output, aux_loss, f_i\n",
    "\n",
    "class GPTWithTracking(GPT):\n",
    "    \"\"\"Track expert assignments across layers\"\"\"\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        device = idx.device\n",
    "        b, t = idx.size()\n",
    "        assert t <= self.config.n_ctx, f\"Cannot forward sequence of length {t}, context length is only {self.config.n_ctx}\"\n",
    "        pos = torch.arange(0, t, dtype=torch.long, device=device)\n",
    "\n",
    "        tok_emb = self.transformer.wte(idx)\n",
    "        pos_emb = self.transformer.wpe(pos)\n",
    "        x = self.transformer.drop(tok_emb + pos_emb)\n",
    "        \n",
    "        # Initialize tracking structures\n",
    "        combined_aux_loss = {}\n",
    "        all_expert_usage = []\n",
    "        all_expert_assignments = {}\n",
    "        all_router_logits = {}\n",
    "        all_router_probs = {}\n",
    "        aux_loss_count = 0\n",
    "\n",
    "        for layer_idx, block in enumerate(self.transformer.h):\n",
    "            block_out = block(x)\n",
    "            \n",
    "            x, aux_loss, f_i = block_out\n",
    "            \n",
    "            if f_i is not None:\n",
    "                all_expert_usage.append(f_i)\n",
    "            \n",
    "            all_expert_assignments[f'layer_{layer_idx}'] = aux_loss['expert_assignments']\n",
    "            all_router_logits[f'layer_{layer_idx}'] = aux_loss['router_logits']\n",
    "            all_router_probs[f'layer_{layer_idx}'] = aux_loss['router_probs']\n",
    "            \n",
    "            if layer_idx == 0:\n",
    "                combined_aux_loss = {k: v.clone() for k, v in aux_loss.items() \n",
    "                                if k not in ['expert_assignments', 'router_logits', 'router_probs']}\n",
    "            else:\n",
    "                for key in aux_loss:\n",
    "                    if key not in ['expert_assignments', 'router_logits', 'router_probs']:\n",
    "                        combined_aux_loss[key] += aux_loss[key]\n",
    "            \n",
    "            aux_loss_count += 1\n",
    "\n",
    "        for key in combined_aux_loss:\n",
    "            combined_aux_loss[key] /= aux_loss_count\n",
    "\n",
    "        if all_expert_usage:  # Check in case all f_i were None\n",
    "            avg_expert_usage = torch.stack(all_expert_usage).mean(dim=0)\n",
    "            combined_aux_loss['expert_usage'] = avg_expert_usage\n",
    "\n",
    "        combined_aux_loss['expert_assignments'] = all_expert_assignments\n",
    "        combined_aux_loss['router_logits'] = all_router_logits\n",
    "        combined_aux_loss['router_probs'] = all_router_probs\n",
    "\n",
    "        x = self.transformer.ln_f(x)\n",
    "        if targets is not None:\n",
    "            logits = self.lm_head(x)\n",
    "            ce_loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)\n",
    "            loss = ce_loss\n",
    "            if combined_aux_loss is not None:\n",
    "                loss = loss + self.config.load_balance_loss_weight * combined_aux_loss['load_balance_loss'] + self.config.router_z_loss_weight * combined_aux_loss['router_z_loss']\n",
    "                combined_aux_loss['ce_loss'] = ce_loss\n",
    "        else:\n",
    "            logits = self.lm_head(x[:, [-1], :])\n",
    "            loss = None\n",
    "            ce_loss = None\n",
    "\n",
    "        return logits, loss, combined_aux_loss\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the three different seed checkpoints\n",
    "checkpoint_path = \"out-openwebtext/moe-8x2-variable-4x2944-4x128-seed1337/ckpt.pt\"\n",
    "# checkpoint_path = \"out-openwebtext/moe-8x2-variable-4x2944-4x128-seed1223/ckpt.pt\"\n",
    "# checkpoint_path = \"out-openwebtext/moe-8x2-variable-4x2944-4x128-seed42/ckpt.pt\"\n",
    "\n",
    "model = GPTWithTracking(config)\n",
    "\n",
    "for block in model.transformer.h:\n",
    "    if hasattr(block.mlp, 'expert_sizes'):\n",
    "        old_mlp = block.mlp\n",
    "        block.mlp = MoeMLPWithTracking(config)\n",
    "        block.mlp.load_state_dict(old_mlp.state_dict())\n",
    "\n",
    "checkpoint = torch.load(checkpoint_path, map_location='cpu')\n",
    "\n",
    "state_dict = checkpoint['model']\n",
    "if any(k.startswith('_orig_mod.') for k in state_dict.keys()):\n",
    "    state_dict = {k.replace('_orig_mod.', ''): v for k, v in state_dict.items()}\n",
    "\n",
    "model.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collect routing statistics across the entire validation set per layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "\n",
    "tokenizer = tiktoken.get_encoding('gpt2')\n",
    "\n",
    "val_data_path = \"data/openwebtext/val.bin\"\n",
    "val_data = np.memmap(val_data_path, dtype=np.uint16, mode='r')\n",
    "\n",
    "device = 'cuda'\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "num_layers = config.n_layer\n",
    "token_stats_per_layer = {}\n",
    "expert_sizes = model.transformer.h[0].mlp.expert_sizes\n",
    "\n",
    "for layer_idx in range(num_layers):\n",
    "    token_stats_per_layer[f'layer_{layer_idx}'] = defaultdict(lambda: {\n",
    "        'expert_counts': np.zeros(config.num_experts, dtype=np.int64),\n",
    "        'total_occurrences': 0,\n",
    "        'total_entropy': 0.0,\n",
    "        'expert_size_sum': 0.0,\n",
    "    })\n",
    "\n",
    "batch_size = 8\n",
    "seq_len = 1024\n",
    "total_tokens = len(val_data)\n",
    "num_batches = total_tokens // seq_len\n",
    "\n",
    "for batch_idx in tqdm(range(num_batches)):\n",
    "    start_idx = batch_idx * seq_len\n",
    "    end_idx = start_idx + seq_len\n",
    "    batch_tokens = torch.from_numpy(val_data[start_idx:end_idx].astype(np.int64)).unsqueeze(0).to(device)\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        logits, loss, aux_loss = model(batch_tokens, targets=batch_tokens)\n",
    "    \n",
    "    output_probs = F.softmax(logits[0], dim=-1) \n",
    "    epsilon = 1e-10 # can't do log(0) so need epsilon\n",
    "    output_entropy = -(output_probs * torch.lot(output_probs + epsilon)).sum(dim=-1).cpu().numpy()\n",
    "\n",
    "    for layer_idx in range(num_layers):\n",
    "        layer_name = f'layer_{layer_idx}'\n",
    "        layer_assignments = aux_loss['expert_assignments'][layer_name][0].cpu().numpy()  # (seq_len, k)\n",
    "        token_stats = token_stats_per_layer[layer_name]\n",
    "        \n",
    "        # Update statistics for each token\n",
    "        for pos in range(seq_len):\n",
    "            token_id = int(batch_tokens[0, pos].item())\n",
    "            expert_ids = layer_assignments[pos]  # k experts for this position\n",
    "            \n",
    "            # Update counts\n",
    "            token_stats[token_id]['total_occurrences'] += 1\n",
    "            token_stats[token_id]['total_entropy'] += output_entropy[pos]\n",
    "            \n",
    "            # Track which experts were used\n",
    "            for expert_id in expert_ids:\n",
    "                token_stats[token_id]['expert_counts'][expert_id] += 1\n",
    "                token_stats[token_id]['expert_size_sum'] += expert_sizes[expert_id]\n",
    "\n",
    "print(f\"\\nCollected statistics for each layer\")\n",
    "for layer_name, token_stats in token_stats_per_layer.items():\n",
    "    num_unique_tokens = len(token_stats)\n",
    "    total_occurrences = sum(s['total_occurrences'] for s in token_stats.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Track expert COMBINATIONS per layer (not just individual counts)\n",
    "from collections import Counter\n",
    "\n",
    "# Initialize combination tracking for each layer\n",
    "token_combinations_per_layer = {}\n",
    "for layer_idx in range(num_layers):\n",
    "    token_combinations_per_layer[f'layer_{layer_idx}'] = defaultdict(Counter)\n",
    "\n",
    "print(f\"Tracking expert combinations for {num_batches} batches across {num_layers} layers...\")\n",
    "\n",
    "# Re-process to track combinations\n",
    "for batch_idx in tqdm(range(num_batches)):\n",
    "    start_idx = batch_idx * seq_len\n",
    "    end_idx = start_idx + seq_len\n",
    "    batch_tokens = torch.from_numpy(val_data[start_idx:end_idx].astype(np.int64)).unsqueeze(0).to(device)\n",
    "    \n",
    "    with torch.inference_mode():\n",
    "        logits, loss, aux_loss = model(batch_tokens, targets=batch_tokens)\n",
    "    \n",
    "    # Track combinations for each layer\n",
    "    for layer_idx in range(num_layers):\n",
    "        layer_name = f'layer_{layer_idx}'\n",
    "        layer_assignments = aux_loss['expert_assignments'][layer_name][0].cpu().numpy()\n",
    "        \n",
    "        for pos in range(seq_len):\n",
    "            token_id = int(batch_tokens[0, pos].item())\n",
    "            expert_ids = tuple(sorted(layer_assignments[pos]))  # Sort for consistency\n",
    "            token_combinations_per_layer[layer_name][token_id][expert_ids] += 1\n",
    "\n",
    "print(\"\\nDone tracking combinations!\")\n",
    "\n",
    "# Now print the most common combination for each token across all layers\n",
    "print(\"\\n\" + \"=\"*200)\n",
    "print(f\"{'Token ID':<10} {'Token':<20} {'Avg Size':<12} {'FLOPs':<15} {'Layer 0':<20} {'Layer 1':<20} {'Layer 2':<20} {'Layer 3':<20} {'Layer 4':<20} {'Layer 5':<20} {'Layer 6':<20} {'Layer 7':<20}\")\n",
    "print(\"=\"*200)\n",
    "\n",
    "# Get unique tokens that appeared in any layer\n",
    "all_token_ids = set()\n",
    "for layer_combos in token_combinations_per_layer.values():\n",
    "    all_token_ids.update(layer_combos.keys())\n",
    "\n",
    "# Track counts for summary\n",
    "large_expert_tokens = 0\n",
    "small_expert_tokens = 0\n",
    "total_flops = 0\n",
    "\n",
    "# Sort by token ID for consistent ordering\n",
    "for token_id in sorted(all_token_ids):  # Show ALL tokens\n",
    "    # Decode token - handle byte-level tokens that don't decode cleanly\n",
    "    try:\n",
    "        token_text = tokenizer.decode([token_id])\n",
    "        # Replace problematic characters\n",
    "        token_text = token_text.replace('\\n', '\\\\n').replace('\\t', '\\\\t').replace('\\r', '\\\\r')\n",
    "        # Check if it's a weird byte-level token\n",
    "        if '�' in token_text or not token_text.isprintable():\n",
    "            # Show the token ID representation instead\n",
    "            token_text = f\"<{token_id}>\"\n",
    "        if len(token_text) > 18:\n",
    "            token_text = token_text[:17] + '…'\n",
    "    except:\n",
    "        token_text = f\"<{token_id}>\"\n",
    "    \n",
    "    # Calculate average expert SIZE across all layers\n",
    "    total_size = 0\n",
    "    layer_count = 0\n",
    "    for layer_idx in range(num_layers):\n",
    "        layer_name = f'layer_{layer_idx}'\n",
    "        combos = token_combinations_per_layer[layer_name][token_id]\n",
    "        if combos:\n",
    "            most_common = combos.most_common(1)[0][0]\n",
    "            # Sum the sizes of the experts in this layer\n",
    "            layer_size = sum(expert_sizes[e] for e in most_common)\n",
    "            total_size += layer_size\n",
    "            layer_count += 1\n",
    "    \n",
    "    avg_size = total_size / layer_count if layer_count > 0 else 0\n",
    "    \n",
    "    # Calculate FLOPs: 4 * hidden_size * total_size\n",
    "    flops = 4 * config.n_embd * total_size\n",
    "    total_flops += flops\n",
    "    # Count for summary\n",
    "    if avg_size >= 2560:\n",
    "        large_expert_tokens += 1\n",
    "    else:\n",
    "        small_expert_tokens += 1\n",
    "    \n",
    "    # Get most common combination for each layer\n",
    "    row = [f\"{token_id:<10}\", f\"{token_text:<20}\", f\"{avg_size:<12.1f}\", f\"{flops:<15,}\"]\n",
    "    for layer_idx in range(num_layers):\n",
    "        layer_name = f'layer_{layer_idx}'\n",
    "        combos = token_combinations_per_layer[layer_name][token_id]\n",
    "        \n",
    "        if combos:\n",
    "            most_common = combos.most_common(1)[0][0]  # Get the tuple of experts\n",
    "            # Format with expert sizes: (5(128),7(128))\n",
    "            formatted = \"(\" + \",\".join([f\"{e}({expert_sizes[e]})\" for e in most_common]) + \")\"\n",
    "            row.append(f\"{formatted:<20}\")\n",
    "        else:\n",
    "            row.append(f\"{'N/A':<20}\")\n",
    "    \n",
    "    print(\"\".join(row))\n",
    "\n",
    "print(\"=\"*200)\n",
    "print(f\"\\nTotal unique tokens: {len(all_token_ids)}\")\n",
    "print(f\"\\nAverage Size Summary:\")\n",
    "print(f\"  Tokens with avg size >= 2560 (mostly large experts): {large_expert_tokens} ({100*large_expert_tokens/len(all_token_ids):.2f}%)\")\n",
    "print(f\"  Tokens with avg size < 2560 (mostly small experts):  {small_expert_tokens} ({100*small_expert_tokens/len(all_token_ids):.2f}%)\")\n",
    "print(f\"  Average number of FLOPs per token: {total_flops/len(all_token_ids):.0f} ({100*(total_flops/len(all_token_ids))/(8*4*640*2560):.2f}% of baseline)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Initialize combination tracking for each layer\n",
    "token_combinations_per_layer = {}\n",
    "for layer_idx in range(num_layers):\n",
    "    token_combinations_per_layer[f'layer_{layer_idx}'] = defaultdict(Counter)\n",
    "\n",
    "print(f\"Tracking expert combinations for {num_batches} batches across {num_layers} layers...\")\n",
    "\n",
    "# Re-process to track combinations\n",
    "for batch_idx in tqdm(range(num_batches)):\n",
    "    start_idx = batch_idx * seq_len\n",
    "    end_idx = start_idx + seq_len\n",
    "    batch_tokens = torch.from_numpy(val_data[start_idx:end_idx].astype(np.int64)).unsqueeze(0).to(device)\n",
    "    \n",
    "    with torch.inference_mode():\n",
    "        logits, loss, aux_loss = model(batch_tokens, targets=batch_tokens)\n",
    "    \n",
    "    # Track combinations for each layer\n",
    "    for layer_idx in range(num_layers):\n",
    "        layer_name = f'layer_{layer_idx}'\n",
    "        layer_assignments = aux_loss['expert_assignments'][layer_name][0].cpu().numpy()\n",
    "        \n",
    "        for pos in range(seq_len):\n",
    "            token_id = int(batch_tokens[0, pos].item())\n",
    "            expert_ids = tuple(sorted(layer_assignments[pos]))\n",
    "            token_combinations_per_layer[layer_name][token_id][expert_ids] += 1\n",
    "\n",
    "print(\"\\nDone tracking combinations! Building dataframe...\")\n",
    "\n",
    "# Get unique tokens\n",
    "all_token_ids = set()\n",
    "for layer_combos in token_combinations_per_layer.values():\n",
    "    all_token_ids.update(layer_combos.keys())\n",
    "\n",
    "# Build data for dataframe\n",
    "data = []\n",
    "for token_id in all_token_ids:\n",
    "    # Decode token\n",
    "    try:\n",
    "        token_text = tokenizer.decode([token_id])\n",
    "        token_text = token_text.replace('\\n', '\\\\n').replace('\\t', '\\\\t').replace('\\r', '\\\\r')\n",
    "        if '�' in token_text or not token_text.isprintable():\n",
    "            token_text = f\"<{token_id}>\"\n",
    "        if len(token_text) > 18:\n",
    "            token_text = token_text[:17] + '…'\n",
    "    except:\n",
    "        token_text = f\"<{token_id}>\"\n",
    "    \n",
    "    # Calculate average expert SIZE across all layers\n",
    "    total_size = 0\n",
    "    layer_count = 0\n",
    "    layer_data = {}\n",
    "    \n",
    "    for layer_idx in range(num_layers):\n",
    "        layer_name = f'layer_{layer_idx}'\n",
    "        combos = token_combinations_per_layer[layer_name][token_id]\n",
    "        if combos:\n",
    "            most_common = combos.most_common(1)[0][0]\n",
    "            layer_size = sum(expert_sizes[e] for e in most_common)\n",
    "            total_size += layer_size\n",
    "            layer_count += 1\n",
    "            # Format with expert sizes: (5(128),7(128))\n",
    "            formatted = \"(\" + \",\".join([f\"{e}({expert_sizes[e]})\" for e in most_common]) + \")\"\n",
    "            layer_data[f'layer_{layer_idx}'] = formatted\n",
    "        else:\n",
    "            layer_data[f'layer_{layer_idx}'] = 'N/A'\n",
    "    \n",
    "    avg_size = total_size / layer_count if layer_count > 0 else 0\n",
    "    flops = 4 * config.n_embd * total_size\n",
    "    \n",
    "    row = {\n",
    "        'token_id': token_id,\n",
    "        'token': token_text,\n",
    "        'avg_size': avg_size,\n",
    "        'flops': flops,\n",
    "        **layer_data\n",
    "    }\n",
    "    data.append(row)\n",
    "\n",
    "# Create DataFrame and sort by FLOPs\n",
    "df = pd.DataFrame(data)\n",
    "df = df.sort_values('flops', ascending=True).reset_index(drop=True)\n",
    "\n",
    "print(f\"\\nDataFrame created with {len(df)} tokens, sorted by FLOPs (low to high)\")\n",
    "print(f\"\\nFirst 20 rows (lowest FLOPs):\")\n",
    "print(df.head(20).to_string())\n",
    "\n",
    "print(f\"\\n\\nLast 20 rows (highest FLOPs):\")\n",
    "print(df.tail(20).to_string())\n",
    "\n",
    "# Summary statistics\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"Summary Statistics:\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"Total unique tokens: {len(df)}\")\n",
    "print(f\"\\nFLOPs distribution:\")\n",
    "print(f\"  Min:    {df['flops'].min():,.0f}\")\n",
    "print(f\"  25%:    {df['flops'].quantile(0.25):,.0f}\")\n",
    "print(f\"  Median: {df['flops'].median():,.0f}\")\n",
    "print(f\"  75%:    {df['flops'].quantile(0.75):,.0f}\")\n",
    "print(f\"  Max:    {df['flops'].max():,.0f}\")\n",
    "print(f\"  Mean:   {df['flops'].mean():,.0f}\")\n",
    "\n",
    "print(f\"\\nAverage Size distribution:\")\n",
    "print(f\"  Tokens with avg_size >= 2560: {(df['avg_size'] >= 2560).sum()} ({100*(df['avg_size'] >= 2560).sum()/len(df):.2f}%)\")\n",
    "print(f\"  Tokens with avg_size < 2560:  {(df['avg_size'] < 2560).sum()} ({100*(df['avg_size'] < 2560).sum()/len(df):.2f}%)\")\n",
    "\n",
    "baseline_flops = 8 * 4 * 640 * 2560  # num_layers * 4 * hidden_size * expert_size\n",
    "print(f\"\\nAverage FLOPs per token: {df['flops'].mean():.0f} ({100*df['flops'].mean()/baseline_flops:.2f}% of baseline)\")\n",
    "\n",
    "# Store the dataframe for further analysis\n",
    "expert_combinations_df = df\n",
    "sweep_value = '-'.join(checkpoint_path.split('-')).split('/')[-2]\n",
    "\n",
    "df.to_csv(f'{sweep_value}_expert_combinations.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze per-token statistics FOR EACH LAYER\n",
    "for layer_idx in range(num_layers):\n",
    "    layer_name = f'layer_{layer_idx}'\n",
    "    token_stats = token_stats_per_layer[layer_name]\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"LAYER {layer_idx} ANALYSIS\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    # Compute derived metrics for each token\n",
    "    token_analysis = {}\n",
    "    \n",
    "    for token_id, stats in token_stats.items():\n",
    "        if stats['total_occurrences'] > 0:\n",
    "            # Average entropy\n",
    "            avg_entropy = stats['total_entropy'] / stats['total_occurrences']\n",
    "            \n",
    "            # Expert distribution (normalized)\n",
    "            expert_distribution = stats['expert_counts'] / stats['expert_counts'].sum()\n",
    "            \n",
    "            # Most common expert\n",
    "            most_common_expert = np.argmax(stats['expert_counts'])\n",
    "            \n",
    "            # Average expert size\n",
    "            avg_expert_size = stats['expert_size_sum'] / stats['expert_counts'].sum()\n",
    "            \n",
    "            token_analysis[token_id] = {\n",
    "                'avg_entropy': avg_entropy,\n",
    "                'occurrences': stats['total_occurrences'],\n",
    "                'expert_distribution': expert_distribution,\n",
    "                'most_common_expert': most_common_expert,\n",
    "                'avg_expert_size': avg_expert_size,\n",
    "            }\n",
    "    \n",
    "    # Plot distribution of average expert sizes\n",
    "    all_expert_sizes = np.array([a['avg_expert_size'] for a in token_analysis.values()])\n",
    "    all_occurrences = np.array([a['occurrences'] for a in token_analysis.values()])\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "    fig.suptitle(f'Layer {layer_idx} Token Routing Analysis', fontsize=16)\n",
    "    \n",
    "    # Unweighted histogram (by unique tokens)\n",
    "    ax1 = axes[0]\n",
    "    ax1.hist(all_expert_sizes, bins=50, alpha=0.7, edgecolor='black')\n",
    "    ax1.axvline(x=128, color='blue', linestyle='--', label='Small (128)', linewidth=2)\n",
    "    ax1.axvline(x=2944, color='red', linestyle='--', label='Large (2944)', linewidth=2)\n",
    "    ax1.axvline(x=np.mean(all_expert_sizes), color='green', linestyle='--', label=f'Mean ({np.mean(all_expert_sizes):.0f})', linewidth=2)\n",
    "    ax1.set_xlabel('Average Expert Size per Token')\n",
    "    ax1.set_ylabel('Number of Unique Tokens')\n",
    "    ax1.set_title('Distribution by Unique Tokens (Unweighted)')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Weighted histogram (by token occurrences)\n",
    "    ax2 = axes[1]\n",
    "    ax2.hist(all_expert_sizes, bins=50, weights=all_occurrences, alpha=0.7, edgecolor='black', color='orange')\n",
    "    ax2.axvline(x=128, color='blue', linestyle='--', label='Small (128)', linewidth=2)\n",
    "    ax2.axvline(x=2944, color='red', linestyle='--', label='Large (2944)', linewidth=2)\n",
    "    weighted_mean = np.average(all_expert_sizes, weights=all_occurrences)\n",
    "    ax2.axvline(x=weighted_mean, color='green', linestyle='--', label=f'Weighted Mean ({weighted_mean:.0f})', linewidth=2)\n",
    "    ax2.set_xlabel('Average Expert Size per Token')\n",
    "    ax2.set_ylabel('Total Token Occurrences')\n",
    "    ax2.set_title('Distribution by Token Occurrences (Weighted)')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Pie chart - weighted breakdown\n",
    "    ax3 = axes[2]\n",
    "    large_expert_occurrences = sum(all_occurrences[all_expert_sizes > 1536])\n",
    "    small_expert_occurrences = sum(all_occurrences[all_expert_sizes <= 1536])\n",
    "    total_occurrences = large_expert_occurrences + small_expert_occurrences\n",
    "    \n",
    "    ax3.pie([large_expert_occurrences, small_expert_occurrences],\n",
    "            labels=['Large experts\\n(>1536)', 'Small experts\\n(≤1536)'],\n",
    "            autopct='%1.1f%%',\n",
    "            colors=['red', 'blue'])\n",
    "    ax3.set_title(f'Token Occurrences by Expert Size\\n(Total: {total_occurrences:,} tokens)')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Statistics\n",
    "    print(f\"Average Expert Size Statistics (Unweighted):\")\n",
    "    print(f\"  Mean: {np.mean(all_expert_sizes):.2f}\")\n",
    "    print(f\"  Median: {np.median(all_expert_sizes):.2f}\")\n",
    "    print(f\"  Std: {np.std(all_expert_sizes):.2f}\")\n",
    "    \n",
    "    print(f\"\\nAverage Expert Size Statistics (Weighted by occurrences):\")\n",
    "    print(f\"  Weighted Mean: {weighted_mean:.2f}\")\n",
    "    \n",
    "    # Count how many tokens go to mostly large vs mostly small experts\n",
    "    large_expert_tokens = sum(1 for s in all_expert_sizes if s > 1536)\n",
    "    small_expert_tokens = sum(1 for s in all_expert_sizes if s <= 1536)\n",
    "    print(f\"\\nUnique Token routing breakdown:\")\n",
    "    print(f\"  Unique tokens routing mostly to LARGE experts: {large_expert_tokens} ({100*large_expert_tokens/len(all_expert_sizes):.1f}%)\")\n",
    "    print(f\"  Unique tokens routing mostly to SMALL experts: {small_expert_tokens} ({100*small_expert_tokens/len(all_expert_sizes):.1f}%)\")\n",
    "    \n",
    "    print(f\"\\nWeighted by occurrences:\")\n",
    "    print(f\"  Token occurrences routed to LARGE experts: {large_expert_occurrences:,} ({100*large_expert_occurrences/total_occurrences:.1f}%)\")\n",
    "    print(f\"  Token occurrences routed to SMALL experts: {small_expert_occurrences:,} ({100*small_expert_occurrences/total_occurrences:.1f}%)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SUMMARY ACROSS ALL LAYERS\")\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
