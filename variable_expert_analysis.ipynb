{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro",
   "metadata": {},
   "source": [
    "# Variable Expert Analysis - Unified Notebook\n",
    "\n",
    "This notebook supports analysis of MoE models with variable-sized experts across different model sizes and expert configurations.\n",
    "\n",
    "## Model Sizes\n",
    "\n",
    "### GPT-2 120M (`DATASET = \"gpt2\"`)\n",
    "- 12 layers, 768 dim, OpenWebText dataset\n",
    "- **5:1 ratio** (`EXPERT_CONFIG = \"5to1\"`): 4x2560 + 4x512 experts\n",
    "  - Available seeds: 42, 1223, 1337\n",
    "- **23:1 ratio** (`EXPERT_CONFIG = \"23to1\"`): 4x2944 + 4x128 experts\n",
    "  - Available seeds: 42, 1223, 1337\n",
    "\n",
    "### GPT-2 250M (`DATASET = \"gpt2_250m\"`)\n",
    "- 16 layers, 1024 dim, OpenWebText dataset (Chinchilla scaling)\n",
    "- **5:1 ratio** (`EXPERT_CONFIG = \"5to1\"`): 4x3456 + 4x640 experts\n",
    "  - Available seeds: 1337\n",
    "- **31:1 ratio** (`EXPERT_CONFIG = \"31to1\"`): 4x3968 + 4x128 experts\n",
    "  - Available seeds: 1337\n",
    "- **Uniform baseline** (`EXPERT_CONFIG = \"uniform\"`): 8x2048 experts\n",
    "  - Available seeds: 1337\n",
    "\n",
    "### WikiText (`DATASET = \"wikitext\"`)\n",
    "- 8 layers, 640 dim, WikiText dataset\n",
    "- Single configuration: 4x2432 + 4x128 experts\n",
    "\n",
    "## Usage\n",
    "\n",
    "Change the configuration variables at the top of the config cell:\n",
    "\n",
    "```python\n",
    "DATASET = \"gpt2\"          # Choose model size: \"gpt2\", \"gpt2_250m\", \"wikitext\"\n",
    "EXPERT_CONFIG = \"5to1\"    # Choose expert ratio: \"5to1\", \"23to1\", \"31to1\", \"uniform\"\n",
    "SEED = 1337               # Choose seed: 42, 1223, 1337 (check availability above)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict, Counter\n",
    "from tqdm import tqdm\n",
    "import tiktoken\n",
    "\n",
    "from model import GPTConfig, MoeMLPWithTracking, GPTWithTracking\n",
    "\n",
    "\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "torch.backends.cudnn.allow_tf32 = True\n",
    "torch.set_float32_matmul_precision('high')  # Or 'medium' for even more speed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "config",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Configuration: GPT-2 120M 5:1 (4x2560 + 4x512) seed42\n",
      "Expert sizes: [(4, 2560), (4, 512)]\n",
      "Checkpoint: gpt2_experiments/multiseed_5to1/ratio5_lbl0.01_compute0.004_seed42/ckpt.pt\n",
      "Val data: data/openwebtext/val.bin\n",
      "============================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ========== CONFIGURATION ==========\n",
    "# Change this to switch between model sizes and configurations\n",
    "DATASET = \"gpt2\"  # Options: \"gpt2\" (120M), \"gpt2_250m\" (250M), \"wikitext\"\n",
    "EXPERT_CONFIG = \"23to1\"  # Options: \"5to1\", \"23to1\", \"31to1\", \"uniform\" (depends on DATASET - see availability below)\n",
    "SEED = 42 # Options: 42, 1223, 1337 (depending on which are available)\n",
    "\n",
    "# Dataset-specific configurations\n",
    "CONFIGS = {\n",
    "    \"gpt2\": {\n",
    "        \"n_layer\": 12,\n",
    "        \"n_head\": 12,\n",
    "        \"n_embd\": 768,\n",
    "        \"vocab_size\": 50304,\n",
    "        \"val_data_path\": \"data/openwebtext/val.bin\",\n",
    "        \"expert_configs\": {\n",
    "            \"5to1\": {\n",
    "                \"expert_sizes\": [(4, 2560), (4, 512)],  # 5:1 ratio\n",
    "                \"base_dir\": \"gpt2_experiments/multiseed_5to1\",\n",
    "                \"run_name_pattern\": \"ratio5_lbl0.01_compute0.004_seed{seed}\",\n",
    "                \"model_name\": \"GPT-2 120M 5:1 (4x2560 + 4x512)\",\n",
    "                \"available_seeds\": [42, 1223, 1337]\n",
    "            },\n",
    "            \"23to1\": {\n",
    "                \"expert_sizes\": [(4, 2944), (4, 128)],  # 23:1 ratio\n",
    "                \"base_dir\": \"gpt2_experiments/multiseed_23to1\",\n",
    "                \"run_name_pattern\": \"ratio23_lbl0.01_compute0.004_seed{seed}\",\n",
    "                \"model_name\": \"GPT-2 120M 23:1 (4x2944 + 4x128)\",\n",
    "                \"available_seeds\": [42, 1223, 1337]\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    \"gpt2_250m\": {\n",
    "        \"n_layer\": 16,\n",
    "        \"n_head\": 16,\n",
    "        \"n_embd\": 1024,\n",
    "        \"vocab_size\": 50304,\n",
    "        \"val_data_path\": \"data/openwebtext/val.bin\",\n",
    "        \"expert_configs\": {\n",
    "            \"5to1\": {\n",
    "                \"expert_sizes\": [(4, 3456), (4, 640)],  # 5:1 ratio\n",
    "                \"base_dir\": \"gpt2_250m_experiments/expert_sizes_final_weights\",\n",
    "                \"run_name_pattern\": \"sizes_5to1_lbl0.01_compute0.004_seed{seed}\",\n",
    "                \"model_name\": \"GPT-2 250M 5:1 (4x3456 + 4x640)\",\n",
    "                \"available_seeds\": [1337]\n",
    "            },\n",
    "            \"31to1\": {\n",
    "                \"expert_sizes\": [(4, 3968), (4, 128)],  # 31:1 ratio\n",
    "                \"base_dir\": \"gpt2_250m_experiments/expert_sizes_final_weights\",\n",
    "                \"run_name_pattern\": \"sizes_31to1_lbl0.01_compute0.004_seed{seed}\",\n",
    "                \"model_name\": \"GPT-2 250M 31:1 (4x3968 + 4x128)\",\n",
    "                \"available_seeds\": [1337]\n",
    "            },\n",
    "            \"uniform\": {\n",
    "                \"expert_sizes\": [(8, 2048)],  # 1:1 baseline\n",
    "                \"base_dir\": \"gpt2_250m_experiments/expert_sizes_final_weights\",\n",
    "                \"run_name_pattern\": \"sizes_uniform_lbl0.01_compute0.004_seed{seed}\",\n",
    "                \"model_name\": \"GPT-2 250M Uniform (8x2048)\",\n",
    "                \"available_seeds\": [1337]\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    \"wikitext\": {\n",
    "        \"n_layer\": 8,\n",
    "        \"n_head\": 8,\n",
    "        \"n_embd\": 640,\n",
    "        \"vocab_size\": 8192,\n",
    "        \"expert_sizes\": [(4, 2432), (4, 128)],\n",
    "        \"checkpoint_dir\": f\"out-wikitext/moe-8x2-variable-4x2432-4x128-seed{SEED}\",\n",
    "        \"val_data_path\": \"data/wikitext/val.bin\",\n",
    "        \"model_name\": f\"WikiText (4x2432 + 4x128) seed{SEED}\"\n",
    "    }\n",
    "}\n",
    "\n",
    "# Build configuration based on DATASET\n",
    "if DATASET in [\"gpt2\", \"gpt2_250m\"]:\n",
    "    base_cfg = CONFIGS[DATASET]\n",
    "    expert_cfg = base_cfg[\"expert_configs\"][EXPERT_CONFIG]\n",
    "    \n",
    "    # Check if seed is available\n",
    "    if SEED not in expert_cfg[\"available_seeds\"]:\n",
    "        print(f\"WARNING: Seed {SEED} not available for {DATASET} {EXPERT_CONFIG}\")\n",
    "        print(f\"Available seeds: {expert_cfg['available_seeds']}\")\n",
    "    \n",
    "    # Build run name with seed\n",
    "    if \"{seed}\" in expert_cfg[\"run_name_pattern\"]:\n",
    "        run_name = expert_cfg[\"run_name_pattern\"].replace(\"{seed}\", str(SEED))\n",
    "    else:\n",
    "        run_name = expert_cfg[\"run_name_pattern\"]\n",
    "    \n",
    "    cfg = {\n",
    "        \"n_layer\": base_cfg[\"n_layer\"],\n",
    "        \"n_head\": base_cfg[\"n_head\"],\n",
    "        \"n_embd\": base_cfg[\"n_embd\"],\n",
    "        \"vocab_size\": base_cfg[\"vocab_size\"],\n",
    "        \"expert_sizes\": expert_cfg[\"expert_sizes\"],\n",
    "        \"checkpoint_dir\": f\"{expert_cfg['base_dir']}/{run_name}\",\n",
    "        \"val_data_path\": base_cfg[\"val_data_path\"],\n",
    "        \"model_name\": f\"{expert_cfg['model_name']} seed{SEED}\"\n",
    "    }\n",
    "else:\n",
    "    # Flat config (e.g., wikitext)\n",
    "    cfg = CONFIGS[DATASET]\n",
    "\n",
    "# Create model config\n",
    "config = GPTConfig(\n",
    "    n_layer = cfg['n_layer'],\n",
    "    n_head = cfg['n_head'],\n",
    "    n_embd = cfg['n_embd'],\n",
    "    bias = False,\n",
    "    vocab_size= cfg['vocab_size'],\n",
    "    \n",
    "    # MoE configuration with VARIABLE-SIZE EXPERTS\n",
    "    use_moe = True,\n",
    "    num_experts = 8,\n",
    "    num_experts_per_tok = 2,\n",
    "    norm_topk_prob = True,\n",
    "    block_size = 128,\n",
    "    block_k = 64,\n",
    "    expert_sizes = cfg[\"expert_sizes\"]\n",
    ")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Configuration: {cfg['model_name']}\")\n",
    "print(f\"Expert sizes: {config.expert_sizes}\")\n",
    "print(f\"Checkpoint: {cfg['checkpoint_dir']}/ckpt.pt\")\n",
    "print(f\"Val data: {cfg['val_data_path']}\")\n",
    "print(f\"{'='*60}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "load-model",
   "metadata": {},
   "source": [
    "## Load Model and Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = f\"{cfg['checkpoint_dir']}/ckpt.pt\"\n",
    "\n",
    "model = GPTWithTracking(config).to(torch.bfloat16)\n",
    "\n",
    "for block in model.transformer.h:\n",
    "    if hasattr(block.mlp, 'expert_sizes'):\n",
    "        old_mlp = block.mlp\n",
    "        block.mlp = MoeMLPWithTracking(config).to(torch.bfloat16)\n",
    "        block.mlp.load_state_dict(old_mlp.state_dict())\n",
    "\n",
    "checkpoint = torch.load(checkpoint_path, map_location='cpu')\n",
    "\n",
    "state_dict = checkpoint['model']\n",
    "if any(k.startswith('_orig_mod.') for k in state_dict.keys()):\n",
    "    state_dict = {k.replace('_orig_mod.', ''): v for k, v in state_dict.items()}\n",
    "\n",
    "model.load_state_dict(state_dict)\n",
    "\n",
    "print(f\"✓ Loaded checkpoint from {checkpoint_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "collect-stats",
   "metadata": {},
   "source": [
    "## Collect ALL Routing Statistics in a SINGLE Pass\n",
    "\n",
    "This cell collects all statistics (individual expert assignments and combinations) in one efficient pass through the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "single-pass",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tiktoken.get_encoding('gpt2')\n",
    "\n",
    "val_data_path = cfg['val_data_path']\n",
    "val_data = np.memmap(val_data_path, dtype=np.uint16, mode='r')\n",
    "\n",
    "device = 'cuda'\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "num_layers = config.n_layer\n",
    "expert_sizes = model.transformer.h[0].mlp.expert_sizes\n",
    "\n",
    "# Initialize ALL tracking structures\n",
    "token_stats_per_layer = {}\n",
    "token_combinations_per_layer = {}\n",
    "\n",
    "for layer_idx in range(num_layers):\n",
    "    layer_name = f'layer_{layer_idx}'\n",
    "    token_stats_per_layer[layer_name] = defaultdict(lambda: {\n",
    "        'expert_counts': np.zeros(config.num_experts, dtype=np.int64),\n",
    "        'total_occurrences': 0,\n",
    "        'total_entropy': 0.0,\n",
    "        'total_layer_entropy': 0.0,\n",
    "        'expert_size_sum': 0.0,\n",
    "    })\n",
    "    token_combinations_per_layer[layer_name] = defaultdict(Counter)\n",
    "\n",
    "batch_size = 1\n",
    "seq_len = 1024\n",
    "total_tokens = len(val_data)\n",
    "num_batches = total_tokens // seq_len\n",
    "\n",
    "print(f\"Running single pass through {num_batches} batches to collect all statistics...\")\n",
    "\n",
    "for batch_idx in tqdm(range(num_batches)):\n",
    "    start_idx = batch_idx * seq_len\n",
    "    end_idx = start_idx + seq_len\n",
    "    batch_tokens = torch.from_numpy(val_data[start_idx:end_idx].astype(np.int64)).unsqueeze(0).to(device)\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        logits, loss, aux_loss = model(batch_tokens, targets=batch_tokens)\n",
    "\n",
    "    output_probs = F.softmax(logits[0], dim=-1)\n",
    "    epsilon = 1e-10\n",
    "    output_entropy = -(output_probs * torch.log(output_probs + epsilon)).sum(dim=-1).float().cpu().numpy()\n",
    "\n",
    "    for layer_idx in range(num_layers):\n",
    "        layer_name = f'layer_{layer_idx}'\n",
    "        layer_assignments = aux_loss['expert_assignments'][layer_name][0].cpu().numpy()\n",
    "        layer_entropies = aux_loss['layer_entropies'][layer_name][0].cpu().numpy()\n",
    "        token_stats = token_stats_per_layer[layer_name]\n",
    "\n",
    "        for pos in range(seq_len):\n",
    "            token_id = int(batch_tokens[0, pos].item())\n",
    "            expert_ids = layer_assignments[pos]\n",
    "\n",
    "            # Update individual expert statistics\n",
    "            token_stats[token_id]['total_occurrences'] += 1\n",
    "            token_stats[token_id]['total_entropy'] += output_entropy[pos]\n",
    "            token_stats[token_id]['total_layer_entropy'] += layer_entropies[pos]\n",
    "\n",
    "            for expert_id in expert_ids:\n",
    "                token_stats[token_id]['expert_counts'][expert_id] += 1\n",
    "                token_stats[token_id]['expert_size_sum'] += expert_sizes[expert_id]\n",
    "\n",
    "            # Track expert combinations\n",
    "            expert_combination = tuple(sorted(expert_ids))\n",
    "            token_combinations_per_layer[layer_name][token_id][expert_combination] += 1\n",
    "\n",
    "print(f\"\\n✓ Collected all statistics in a single pass!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "analysis",
   "metadata": {},
   "source": [
    "## Analysis: Print the Most Common Expert Combination for Each Token\n",
    "\n",
    "This shows which expert pair each token uses most frequently across all layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "print-combinations",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_expert_combo(combo, expert_sizes):\n",
    "    \"\"\"Format expert combination with sizes\"\"\"\n",
    "    return \",\".join([f\"{e}({expert_sizes[e]})\" for e in combo])\n",
    "\n",
    "def print_token_routing_table(token_combinations_per_layer, token_stats_per_layer, expert_sizes, tokenizer, max_tokens=100):\n",
    "    \"\"\"Print comprehensive routing table\"\"\"\n",
    "    \n",
    "    # Calculate average expert size and FLOPs for each token\n",
    "    token_avg_sizes = {}\n",
    "    for layer_name in token_stats_per_layer:\n",
    "        for token_id, stats in token_stats_per_layer[layer_name].items():\n",
    "            if stats['total_occurrences'] > 0:\n",
    "                avg_size = stats['expert_size_sum'] / (stats['total_occurrences'] * 2)  # 2 experts per token\n",
    "                if token_id not in token_avg_sizes:\n",
    "                    token_avg_sizes[token_id] = []\n",
    "                token_avg_sizes[token_id].append(avg_size)\n",
    "    \n",
    "    overall_avg_sizes = {tid: np.mean(sizes) for tid, sizes in token_avg_sizes.items()}\n",
    "    \n",
    "    # Header\n",
    "    print(f\"\\n{'='*200}\")\n",
    "    header = f\"{'Token ID':<10}{'Token':<20}{'Avg Size':<13}{'FLOPs':<16}\"\n",
    "    for layer_idx in range(num_layers):\n",
    "        header += f\"{'Layer ' + str(layer_idx):<21}\"\n",
    "    print(header)\n",
    "    print(f\"{'='*200}\")\n",
    "    \n",
    "    # Print first N tokens\n",
    "    for token_id in sorted(overall_avg_sizes.keys())[:max_tokens]:\n",
    "        try:\n",
    "            token_str = tokenizer.decode([token_id]).replace('\\n', '\\\\n')\n",
    "        except:\n",
    "            token_str = f\"<{token_id}>\"\n",
    "        \n",
    "        avg_size = overall_avg_sizes[token_id]\n",
    "        # FLOPs calculation: 2 * seq_len * hidden * expert_size (for matrix mult)\n",
    "        # Simplified: avg_size * hidden * 2 operations\n",
    "        flops = avg_size * config.n_embd * 4 * config.n_layer  # 4x forward/backward\n",
    "        \n",
    "        row = f\"{token_id:<10}{token_str:<20}{avg_size:<13.1f}{flops:<16,.0f}\"\n",
    "        \n",
    "        for layer_idx in range(num_layers):\n",
    "            layer_name = f'layer_{layer_idx}'\n",
    "            if token_id in token_combinations_per_layer[layer_name]:\n",
    "                combos = token_combinations_per_layer[layer_name][token_id]\n",
    "                most_common = combos.most_common(1)[0][0]\n",
    "                combo_str = f\"({format_expert_combo(most_common, expert_sizes)})\"\n",
    "                row += f\"{combo_str:<21}\"\n",
    "            else:\n",
    "                row += f\"{'N/A':<21}\"\n",
    "        \n",
    "        print(row)\n",
    "\n",
    "print_token_routing_table(token_combinations_per_layer, token_stats_per_layer, expert_sizes, tokenizer, max_tokens=350)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a67e83c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "print(\"Building dataframe from collected statistics...\")\n",
    "\n",
    "# Get unique tokens\n",
    "all_token_ids = set()\n",
    "for layer_combos in token_combinations_per_layer.values():\n",
    "    all_token_ids.update(layer_combos.keys())\n",
    "\n",
    "# Build data for dataframe\n",
    "data = []\n",
    "for token_id in all_token_ids:\n",
    "    # Decode token\n",
    "    try:\n",
    "        token_text = tokenizer.decode([token_id])\n",
    "        token_text = token_text.replace('\\n', '\\\\n').replace('\\t', '\\\\t').replace('\\r', '\\\\r')\n",
    "        if '�' in token_text or not token_text.isprintable():\n",
    "            token_text = f\"<{token_id}>\"\n",
    "        if len(token_text) > 18:\n",
    "            token_text = token_text[:17] + '…'\n",
    "    except:\n",
    "        token_text = f\"<{token_id}>\"\n",
    "    \n",
    "    # Calculate average expert SIZE and ENTROPY across all layers\n",
    "    total_size = 0\n",
    "    total_entropy = 0.0\n",
    "    layer_count = 0\n",
    "    layer_data = {}\n",
    "    layer_entropy_data = {}\n",
    "    token_count = 0  # Track total occurrences of this token\n",
    "    \n",
    "    for layer_idx in range(num_layers):\n",
    "        layer_name = f'layer_{layer_idx}'\n",
    "        combos = token_combinations_per_layer[layer_name][token_id]\n",
    "        stats = token_stats_per_layer[layer_name][token_id]\n",
    "        \n",
    "        # Get token count from the first layer (all layers have same count)\n",
    "        if layer_idx == 0 and stats['total_occurrences'] > 0:\n",
    "            token_count = stats['total_occurrences']\n",
    "        \n",
    "        if combos:\n",
    "            most_common = combos.most_common(1)[0][0]\n",
    "            layer_size = sum(expert_sizes[e] for e in most_common)\n",
    "            total_size += layer_size\n",
    "            \n",
    "            # Calculate average entropy for this token in this layer\n",
    "            if stats['total_occurrences'] > 0:\n",
    "                layer_entropy = stats['total_entropy'] / stats['total_occurrences']\n",
    "                total_entropy += layer_entropy\n",
    "                \n",
    "                # Store layer-wise intermediate entropy\n",
    "                layer_wise_entropy = stats['total_layer_entropy'] / stats['total_occurrences']\n",
    "                layer_entropy_data[f'layer_{layer_idx}_entropy'] = layer_wise_entropy\n",
    "            \n",
    "            layer_count += 1\n",
    "            # Format with expert sizes: (5(128),7(128))\n",
    "            formatted = \"(\" + \",\".join([f\"{e}({expert_sizes[e]})\" for e in most_common]) + \")\"\n",
    "            layer_data[f'layer_{layer_idx}'] = formatted\n",
    "        else:\n",
    "            layer_data[f'layer_{layer_idx}'] = 'N/A'\n",
    "            layer_entropy_data[f'layer_{layer_idx}_entropy'] = np.nan\n",
    "    \n",
    "    avg_size = total_size / layer_count if layer_count > 0 else 0\n",
    "    avg_entropy = total_entropy / layer_count if layer_count > 0 else 0\n",
    "    flops = 4 * config.n_embd * total_size\n",
    "    \n",
    "    row = {\n",
    "        'token_id': token_id,\n",
    "        'token': token_text,\n",
    "        'count': token_count,\n",
    "        'avg_size': avg_size,\n",
    "        'avg_entropy': avg_entropy,\n",
    "        'flops': flops,\n",
    "        **layer_data,\n",
    "        **layer_entropy_data\n",
    "    }\n",
    "    data.append(row)\n",
    "\n",
    "# Create DataFrame and sort by FLOPs\n",
    "df = pd.DataFrame(data)\n",
    "df = df.sort_values('flops', ascending=True).reset_index(drop=True)\n",
    "\n",
    "# Calculate weighted FLOPs based on token frequency\n",
    "total_dataset_flops = (df['flops'] * df['count']).sum()\n",
    "total_tokens = df['count'].sum()\n",
    "weighted_avg_flops = total_dataset_flops / total_tokens\n",
    "\n",
    "print(f\"\\nDataFrame created with {len(df)} tokens, sorted by FLOPs (low to high)\")\n",
    "print(f\"\\nFirst 20 rows (lowest FLOPs):\")\n",
    "# Select a subset of columns for display\n",
    "display_cols = ['token_id', 'token', 'count', 'avg_size', 'avg_entropy', 'flops', \n",
    "                'layer_0_entropy', 'layer_5_entropy', 'layer_11_entropy']\n",
    "print(df[display_cols].head(20).to_string())\n",
    "\n",
    "print(f\"\\n\\nLast 20 rows (highest FLOPs):\")\n",
    "print(df[display_cols].tail(20).to_string())\n",
    "\n",
    "# Summary statistics\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"Summary Statistics:\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"Total unique tokens: {len(df)}\")\n",
    "print(f\"Total token occurrences: {total_tokens:,}\")\n",
    "\n",
    "print(f\"\\nToken Count distribution:\")\n",
    "print(f\"  Min:    {df['count'].min():,}\")\n",
    "print(f\"  25%:    {df['count'].quantile(0.25):,.0f}\")\n",
    "print(f\"  Median: {df['count'].median():,.0f}\")\n",
    "print(f\"  75%:    {df['count'].quantile(0.75):,.0f}\")\n",
    "print(f\"  Max:    {df['count'].max():,}\")\n",
    "print(f\"  Mean:   {df['count'].mean():,.0f}\")\n",
    "\n",
    "print(f\"\\nFLOPs distribution:\")\n",
    "print(f\"  Min:    {df['flops'].min():,.0f}\")\n",
    "print(f\"  25%:    {df['flops'].quantile(0.25):,.0f}\")\n",
    "print(f\"  Median: {df['flops'].median():,.0f}\")\n",
    "print(f\"  75%:    {df['flops'].quantile(0.75):,.0f}\")\n",
    "print(f\"  Max:    {df['flops'].max():,.0f}\")\n",
    "print(f\"  Mean:   {df['flops'].mean():,.0f}\")\n",
    "\n",
    "print(f\"\\nAverage Size distribution:\")\n",
    "print(f\"  Tokens with avg_size >= {4*config.n_embd}: {(df['avg_size'] >= 4*config.n_embd).sum()} ({100*(df['avg_size'] >= config.n_embd).sum()/len(df):.2f}%)\")\n",
    "print(f\"  Tokens with avg_size < {4*config.n_embd}:  {(df['avg_size'] < 4*config.n_embd).sum()} ({100*(df['avg_size'] < 4*config.n_embd).sum()/len(df):.2f}%)\")\n",
    "\n",
    "print(f\"\\nAverage Output Entropy distribution:\")\n",
    "print(f\"  Min:    {df['avg_entropy'].min():.4f}\")\n",
    "print(f\"  25%:    {df['avg_entropy'].quantile(0.25):.4f}\")\n",
    "print(f\"  Median: {df['avg_entropy'].median():.4f}\")\n",
    "print(f\"  75%:    {df['avg_entropy'].quantile(0.75):.4f}\")\n",
    "print(f\"  Max:    {df['avg_entropy'].max():.4f}\")\n",
    "print(f\"  Mean:   {df['avg_entropy'].mean():.4f}\")\n",
    "\n",
    "print(f\"\\nLayer-wise Entropy Statistics:\")\n",
    "for layer_idx in [0, num_layers//2, num_layers-1]:\n",
    "    layer_col = f'layer_{layer_idx}_entropy'\n",
    "    print(f\"  Layer {layer_idx}:\")\n",
    "    print(f\"    Mean:   {df[layer_col].mean():.4f}\")\n",
    "    print(f\"    Median: {df[layer_col].median():.4f}\")\n",
    "    print(f\"    Std:    {df[layer_col].std():.4f}\")\n",
    "\n",
    "baseline_flops = config.n_layer * 4 * config.n_embd * (config.n_embd*4)  # num_layers * 4 * hidden_size * total expert_size\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"FLOPs Analysis:\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"Average FLOPs per unique token: {df['flops'].mean():.0f} ({100*df['flops'].mean()/baseline_flops:.2f}% of baseline)\")\n",
    "print(f\"Weighted average FLOPs per token occurrence: {weighted_avg_flops:.0f} ({100*weighted_avg_flops/baseline_flops:.2f}% of baseline)\")\n",
    "print(f\"Total dataset FLOPs: {total_dataset_flops:,.0f}\")\n",
    "print(f\"Baseline total FLOPs: {baseline_flops * total_tokens:,.0f}\")\n",
    "print(f\"FLOPs savings: {100 * (1 - total_dataset_flops / (baseline_flops * total_tokens)):.2f}%\")\n",
    "\n",
    "# Store the dataframe for further analysis\n",
    "expert_combinations_df = df\n",
    "sweep_value = '-'.join(checkpoint_path.split('-')).split('/')[-2]\n",
    "\n",
    "df.to_csv(f'analysis_csvs/{sweep_value}_expert_combinations.csv', index=False)\n",
    "print(f\"\\n✓ Saved to analysis_csvs/{sweep_value}_expert_combinations.csv\")\n",
    "print(f\"  Columns: {', '.join(df.columns.tolist()[:10])}... (and {len(df.columns)-10} more)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc0c44bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze per-token statistics FOR EACH LAYER\n",
    "for layer_idx in range(num_layers):\n",
    "    layer_name = f'layer_{layer_idx}'\n",
    "    token_stats = token_stats_per_layer[layer_name]\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"LAYER {layer_idx} ANALYSIS\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    # Compute derived metrics for each token\n",
    "    token_analysis = {}\n",
    "    \n",
    "    for token_id, stats in token_stats.items():\n",
    "        if stats['total_occurrences'] > 0:\n",
    "            # Average entropy\n",
    "            avg_entropy = stats['total_entropy'] / stats['total_occurrences']\n",
    "            \n",
    "            # Expert distribution (normalized)\n",
    "            expert_distribution = stats['expert_counts'] / stats['expert_counts'].sum()\n",
    "            \n",
    "            # Most common expert\n",
    "            most_common_expert = np.argmax(stats['expert_counts'])\n",
    "            \n",
    "            # Average expert size\n",
    "            avg_expert_size = stats['expert_size_sum'] / stats['expert_counts'].sum()\n",
    "            \n",
    "            token_analysis[token_id] = {\n",
    "                'avg_entropy': avg_entropy,\n",
    "                'occurrences': stats['total_occurrences'],\n",
    "                'expert_distribution': expert_distribution,\n",
    "                'most_common_expert': most_common_expert,\n",
    "                'avg_expert_size': avg_expert_size,\n",
    "            }\n",
    "    \n",
    "    # Plot distribution of average expert sizes\n",
    "    all_expert_sizes = np.array([a['avg_expert_size'] for a in token_analysis.values()])\n",
    "    all_occurrences = np.array([a['occurrences'] for a in token_analysis.values()])\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "    fig.suptitle(f'Layer {layer_idx} Token Routing Analysis', fontsize=16)\n",
    "    \n",
    "    # Unweighted histogram (by unique tokens)\n",
    "    ax1 = axes[0]\n",
    "    ax1.hist(all_expert_sizes, bins=50, alpha=0.7, edgecolor='black')\n",
    "    ax1.axvline(x=config.expert_sizes[1][1], color='blue', linestyle='--', label=f'Small ({config.expert_sizes[1][1]})', linewidth=2)\n",
    "    ax1.axvline(x=config.expert_sizes[0][1], color='red', linestyle='--', label=f'Large ({config.expert_sizes[0][1]})', linewidth=2)\n",
    "    ax1.axvline(x=np.mean(all_expert_sizes), color='green', linestyle='--', label=f'Mean ({np.mean(all_expert_sizes):.0f})', linewidth=2)\n",
    "    ax1.set_xlabel('Average Expert Size per Token')\n",
    "    ax1.set_ylabel('Number of Unique Tokens')\n",
    "    ax1.set_title('Distribution by Unique Tokens (Unweighted)')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Weighted histogram (by token occurrences)\n",
    "    ax2 = axes[1]\n",
    "    ax2.hist(all_expert_sizes, bins=50, weights=all_occurrences, alpha=0.7, edgecolor='black', color='orange')\n",
    "    ax2.axvline(x=config.expert_sizes[1][1], color='blue', linestyle='--', label=f'Small ({config.expert_sizes[1][1]})', linewidth=2)\n",
    "    ax2.axvline(x=config.expert_sizes[0][1], color='red', linestyle='--', label=f'Large ({config.expert_sizes[0][1]})', linewidth=2)\n",
    "    weighted_mean = np.average(all_expert_sizes, weights=all_occurrences)\n",
    "    ax2.axvline(x=weighted_mean, color='green', linestyle='--', label=f'Weighted Mean ({weighted_mean:.0f})', linewidth=2)\n",
    "    ax2.set_xlabel('Average Expert Size per Token')\n",
    "    ax2.set_ylabel('Total Token Occurrences')\n",
    "    ax2.set_title('Distribution by Token Occurrences (Weighted)')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Pie chart - weighted breakdown\n",
    "    ax3 = axes[2]\n",
    "    large_expert_occurrences = sum(all_occurrences[all_expert_sizes > (config.n_embd * 4)/2])\n",
    "    small_expert_occurrences = sum(all_occurrences[all_expert_sizes <= (config.n_embd * 4)/2])\n",
    "    total_occurrences = large_expert_occurrences + small_expert_occurrences\n",
    "    \n",
    "    ax3.pie([large_expert_occurrences, small_expert_occurrences],\n",
    "            labels=[f'Large experts\\n(>{(config.n_embd * 4)/2})', f'Small experts\\n(≤{(config.n_embd * 4)/2})'],\n",
    "            autopct='%1.1f%%',\n",
    "            colors=['red', 'blue'])\n",
    "    ax3.set_title(f'Token Occurrences by Expert Size\\n(Total: {total_occurrences:,} tokens)')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Statistics\n",
    "    print(f\"Average Expert Size Statistics (Unweighted):\")\n",
    "    print(f\"  Mean: {np.mean(all_expert_sizes):.2f}\")\n",
    "    print(f\"  Median: {np.median(all_expert_sizes):.2f}\")\n",
    "    print(f\"  Std: {np.std(all_expert_sizes):.2f}\")\n",
    "    \n",
    "    print(f\"\\nAverage Expert Size Statistics (Weighted by occurrences):\")\n",
    "    print(f\"  Weighted Mean: {weighted_mean:.2f}\")\n",
    "    \n",
    "    # Count how many tokens go to mostly large vs mostly small experts\n",
    "    large_expert_tokens = sum(1 for s in all_expert_sizes if s > (config.n_embd * 4)/2)\n",
    "    small_expert_tokens = sum(1 for s in all_expert_sizes if s <= (config.n_embd * 4)/2)\n",
    "    print(f\"\\nUnique Token routing breakdown:\")\n",
    "    print(f\"  Unique tokens routing mostly to LARGE experts: {large_expert_tokens} ({100*large_expert_tokens/len(all_expert_sizes):.1f}%)\")\n",
    "    print(f\"  Unique tokens routing mostly to SMALL experts: {small_expert_tokens} ({100*small_expert_tokens/len(all_expert_sizes):.1f}%)\")\n",
    "    \n",
    "    print(f\"\\nWeighted by occurrences:\")\n",
    "    print(f\"  Token occurrences routed to LARGE experts: {large_expert_occurrences:,} ({100*large_expert_occurrences/total_occurrences:.1f}%)\")\n",
    "    print(f\"  Token occurrences routed to SMALL experts: {small_expert_occurrences:,} ({100*small_expert_occurrences/total_occurrences:.1f}%)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SUMMARY ACROSS ALL LAYERS\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f104f27e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Extract the ratio from EXPERT_CONFIG (e.g., \"5to1\" -> 5, \"23to1\" -> 23)\n",
    "ratio = int(EXPERT_CONFIG.split('to')[0])\n",
    "\n",
    "dfs = {}\n",
    "for seed in [42, 1223, 1337]:\n",
    "    '''UNCOMMENT ME TO USE, MAKE SURE TO MATCH'''\n",
    "    # checkpoint_dir = f\"out-openwebtext/moe-8x2-variable-4x2944-4x128-seed{seed}\n",
    "    # checkpoint_dir = f\"gpt2_experiments/multiseed_23to1_20251102_203832/ratio23_lbl0.01_compute0.004_seed{seed}\"\n",
    "    # sweep_value = f\"ratio23_lbl0.01_compute0.004_seed{seed}\"\n",
    "    # sweep_value = f\"ratio5_lbl0.01_compute0.004_seed{seed}\"\n",
    "    dfs[seed] = pd.read_csv(f'analysis_csvs/{sweep_value}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "22b110ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using ratio: 5\n",
      "Bucket thresholds: tight=235, medium=470, wide=705\n",
      "\n",
      "Exact bucket:                2,136 tokens (4.5%)\n",
      "Tight bucket (±235):  13,079 tokens (27.4%)\n",
      "Medium bucket (±470): 27,740 tokens (58.0%)\n",
      "Wide bucket (±705):   45,064 tokens (94.3%)\n",
      "\n",
      "Saved buckets to:\n",
      "  - analysis_csvs/owt_same_avg_size_tokens_ratio5_bucket_exact.csv\n",
      "  - analysis_csvs/owt_same_avg_size_tokens_ratio5_bucket_235.csv\n",
      "  - analysis_csvs/owt_same_avg_size_tokens_ratio5_bucket_470.csv\n",
      "  - analysis_csvs/owt_same_avg_size_tokens_ratio5_bucket_705.csv\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Merge all three seeds\n",
    "df = dfs[42].merge(dfs[1223], on='token', suffixes=('_42', '_1223'))\n",
    "df = df.merge(dfs[1337], on='token')\n",
    "df = df.rename(columns={'avg_size': 'avg_size_1337'})\n",
    "\n",
    "# Calculate range (max - min) across all three seeds\n",
    "df['max_size'] = df[['avg_size_42', 'avg_size_1223', 'avg_size_1337']].max(axis=1)\n",
    "df['min_size'] = df[['avg_size_42', 'avg_size_1223', 'avg_size_1337']].min(axis=1)\n",
    "df['range'] = df['max_size'] - df['min_size']\n",
    "df['mean_size'] = df[['avg_size_42', 'avg_size_1223', 'avg_size_1337']].mean(axis=1)\n",
    "\n",
    "# Use ratio to calculate bucket thresholds (scale from baseline of ratio=5)\n",
    "# For ratio=5: tight=235, medium=470, wide=705\n",
    "# Scale proportionally for other ratios\n",
    "tight_threshold = 235\n",
    "medium_threshold = 470\n",
    "wide_threshold = 705\n",
    "\n",
    "# Create different bucket sizes\n",
    "df['all_exact'] = (\n",
    "    np.isclose(df['avg_size_42'], df['avg_size_1223']) & \n",
    "    np.isclose(df['avg_size_1223'], df['avg_size_1337'])\n",
    ")\n",
    "exact_bucket = df[df['all_exact']].copy()\n",
    "tight_bucket = df[df['range'] <= tight_threshold].copy()\n",
    "medium_bucket = df[df['range'] <= medium_threshold].copy()\n",
    "wide_bucket = df[df['range'] <= wide_threshold].copy()\n",
    "\n",
    "print(f\"Using ratio: {ratio}\")\n",
    "print(f\"Bucket thresholds: tight={tight_threshold:.0f}, medium={medium_threshold:.0f}, wide={wide_threshold:.0f}\\n\")\n",
    "\n",
    "print(f\"Exact bucket:                {len(exact_bucket):,} tokens ({100*len(exact_bucket)/len(df):.1f}%)\")\n",
    "print(f\"Tight bucket (±{tight_threshold:.0f}):  {len(tight_bucket):,} tokens ({100*len(tight_bucket)/len(df):.1f}%)\")\n",
    "print(f\"Medium bucket (±{medium_threshold:.0f}): {len(medium_bucket):,} tokens ({100*len(medium_bucket)/len(df):.1f}%)\")\n",
    "print(f\"Wide bucket (±{wide_threshold:.0f}):   {len(wide_bucket):,} tokens ({100*len(wide_bucket)/len(df):.1f}%)\")\n",
    "\n",
    "# Save to CSV with ratio-based filenames\n",
    "exact_bucket.to_csv(f'analysis_csvs/owt_same_avg_size_tokens_ratio{ratio}_bucket_exact.csv', index=False)\n",
    "tight_bucket.to_csv(f'analysis_csvs/owt_same_avg_size_tokens_ratio{ratio}_bucket_{tight_threshold:.0f}.csv', index=False)\n",
    "medium_bucket.to_csv(f'analysis_csvs/owt_same_avg_size_tokens_ratio{ratio}_bucket_{medium_threshold:.0f}.csv', index=False)\n",
    "wide_bucket.to_csv(f'analysis_csvs/owt_same_avg_size_tokens_ratio{ratio}_bucket_{wide_threshold:.0f}.csv', index=False)\n",
    "\n",
    "print(\"\\nSaved buckets to:\")\n",
    "print(f\"  - analysis_csvs/owt_same_avg_size_tokens_ratio{ratio}_bucket_exact.csv\")\n",
    "print(f\"  - analysis_csvs/owt_same_avg_size_tokens_ratio{ratio}_bucket_{tight_threshold:.0f}.csv\")\n",
    "print(f\"  - analysis_csvs/owt_same_avg_size_tokens_ratio{ratio}_bucket_{medium_threshold:.0f}.csv\")\n",
    "print(f\"  - analysis_csvs/owt_same_avg_size_tokens_ratio{ratio}_bucket_{wide_threshold:.0f}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33c11816",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_vanilla = pd.read_csv('analysis_csvs/moe-8x2-8x1536_expert_combinations.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fd635c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "exact_bucket_deduped = exact_bucket.drop_duplicates(subset='token')\n",
    "df_vanilla_deduped = df_vanilla.drop_duplicates(subset='token')\n",
    "\n",
    "result = exact_bucket_deduped.merge(df_vanilla_deduped, on='token', suffixes=('_exact', '_vanilla'))\n",
    "result.to_csv('analysis_csvs/owt_same_avg_size_tokens_bucket_exact_vs_vanilla.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1ec3acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df = pd.read_csv('analysis_csvs/variable_experts_gpt2_data.csv')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da44f9a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df = pd.read_csv('analysis_csvs/variable_experts_gpt2_data.csv')\n",
    "\n",
    "df.plot(x='avg expert size', y='val/ce_loss', kind='scatter')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c85799de",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbf915f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv('analysis_csvs/variable_experts_gpt2_data.csv')\n",
    "\n",
    "# Create ratio labels manually based on the expert sizes\n",
    "ratio_map = {\n",
    "    'Baseline': '1:1',\n",
    "    '2560x512': '5:1',\n",
    "    '2688x384': '7:1',\n",
    "    '2816x256': '11:1',\n",
    "    '2944x128': '23:1'\n",
    "}\n",
    "df['ratio_label'] = df['Name'].map(ratio_map)\n",
    "\n",
    "# Create numeric ratio for sorting/coloring\n",
    "ratio_numeric = {\n",
    "    'Baseline': 1,\n",
    "    '2560x512': 5,\n",
    "    '2688x384': 7,\n",
    "    '2816x256': 11,\n",
    "    '2944x128': 23\n",
    "}\n",
    "df['ratio'] = df['Name'].map(ratio_numeric)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "\n",
    "ax.scatter(df['avg expert size'], df['val/ce_loss'], \n",
    "           c=df['ratio'], s=150, alpha=0.8, edgecolors='black', linewidth=1.5)\n",
    "\n",
    "\n",
    "for idx, row in df.iterrows():\n",
    "    ax.annotate(row['ratio_label'], \n",
    "                (row['avg expert size'], row['val/ce_loss']),\n",
    "                xytext=(8, 8), textcoords='offset points',\n",
    "                fontsize=11, fontweight='bold')\n",
    "\n",
    "\n",
    "baseline_loss = df[df['Name'] == 'Baseline']['val/ce_loss'].values[0]\n",
    "baseline_params = df[df['Name'] == 'Baseline']['avg expert size'].values[0]\n",
    "ax.axhline(baseline_loss, color='gray', linestyle='--', alpha=0.5, linewidth=1)\n",
    "ax.text(1500, baseline_loss + 0.002, 'Baseline', \n",
    "        ha='right', va='bottom', fontsize=9, color='gray')\n",
    "\n",
    "\n",
    "ax.set_xlabel('Average Active Parameters per Token', fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel('Validation Loss', fontsize=12, fontweight='bold')\n",
    "ax.set_title('MoE Efficiency: Loss vs Active Parameters', fontsize=14, fontweight='bold', pad=20)\n",
    "ax.grid(True, alpha=0.3, linestyle=':', linewidth=0.5)\n",
    "ax.set_ylim(3.22, 3.31)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('loss_vs_active_params.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
