{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro",
   "metadata": {},
   "source": [
    "# Variable Expert Analysis - Unified Notebook\n",
    "\n",
    "This notebook supports both model sizes:\n",
    "- **OpenWebText**: 4x2944 + 4x128 experts\n",
    "- **WikiText**: 4x2432 + 4x128 experts\n",
    "\n",
    "Simply change the `DATASET` variable in the config cell to switch between them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from einops import rearrange\n",
    "from collections import defaultdict, Counter\n",
    "from tqdm import tqdm\n",
    "import tiktoken\n",
    "\n",
    "import stk\n",
    "import stk.ops\n",
    "import stk.random\n",
    "import stk.matrix\n",
    "from megablocks.layers.gelu import gelu\n",
    "\n",
    "from model import GPT, GPTConfig, MoeMLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "config",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== CONFIGURATION ==========\n",
    "# Change this to switch between datasets and model sizes\n",
    "DATASET = \"wikitext\"  # Options: \"openwebtext\" or \"wikitext\"\n",
    "SEED = 42  # Options: 42, 1223, 1337\n",
    "\n",
    "# Dataset-specific configurations\n",
    "CONFIGS = {\n",
    "    \"openwebtext\": {\n",
    "        \"n_layer\": 12,\n",
    "        \"n_head\": 12,\n",
    "        \"n_embd\": 768,\n",
    "        \"vocab_size\": 50304,\n",
    "        \"expert_sizes\": [(4, 2944), (4, 128)],  # 4 large (2944) + 4 small (128)\n",
    "        \"checkpoint_dir\": f\"out-openwebtext/moe-8x2-variable-4x2944-4x128-seed{SEED}\",\n",
    "        \"val_data_path\": \"data/openwebtext/val.bin\",\n",
    "        \"model_name\": \"OpenWebText (4x2944 + 4x128)\"\n",
    "    },\n",
    "    \"wikitext\": {\n",
    "        \"n_layer\": 8,\n",
    "        \"n_head\": 8,\n",
    "        \"n_embd\": 640,\n",
    "        \"vocab_size\": 8192,\n",
    "        \"expert_sizes\": [(4, 2432), (4, 128)],  # 4 large (2432) + 4 small (128)\n",
    "        \"checkpoint_dir\": f\"out-wikitext/moe-8x2-variable-4x2432-4x128-seed{SEED}\",\n",
    "        \"val_data_path\": \"data/wikitext/val.bin\",\n",
    "        \"model_name\": \"WikiText (4x2432 + 4x128)\"\n",
    "    }\n",
    "}\n",
    "\n",
    "# Select configuration\n",
    "cfg = CONFIGS[DATASET]\n",
    "\n",
    "# Create model config\n",
    "config = GPTConfig(\n",
    "    n_layer = cfg['n_layer'],\n",
    "    n_head = cfg['n_head'],\n",
    "    n_embd = cfg['n_embd'],\n",
    "    bias = False,\n",
    "    vocab_size= cfg['vocab_size'],\n",
    "    \n",
    "    # MoE configuration with VARIABLE-SIZE EXPERTS\n",
    "    use_moe = True,\n",
    "    num_experts = 8,\n",
    "    num_experts_per_tok = 2,\n",
    "    norm_topk_prob = True,\n",
    "    block_size = 128,\n",
    "    block_k = 64,\n",
    "    expert_sizes = cfg[\"expert_sizes\"]\n",
    ")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Configuration: {cfg['model_name']}\")\n",
    "print(f\"Seed: {SEED}\")\n",
    "print(f\"Expert sizes: {config.expert_sizes}\")\n",
    "print(f\"Checkpoint: {cfg['checkpoint_dir']}/ckpt.pt\")\n",
    "print(f\"Val data: {cfg['val_data_path']}\")\n",
    "print(f\"{'='*60}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tracking-classes",
   "metadata": {},
   "source": [
    "## Subclass the MoE MLP layer and GPT layer to track token routing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "moe-tracking",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MoeMLPWithTracking(MoeMLP):\n",
    "    \"\"\"Add expert assignment tracking to the mlp layer's forward pass\"\"\"\n",
    "\n",
    "    @torch.compiler.disable\n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, n_embd = x.shape\n",
    "\n",
    "        x_flat = rearrange(x, 'batch_size seq_len n_embd -> (batch_size seq_len) n_embd')\n",
    "\n",
    "        router_logits = self.router(x_flat)\n",
    "        router_probs = F.softmax(router_logits, dim=-1, dtype=torch.float32)\n",
    "        expert_weights, selected_experts = torch.topk(router_probs, self.num_experts_per_tok, dim=-1)\n",
    "\n",
    "        if self.norm_topk_prob:\n",
    "            expert_weights = expert_weights / expert_weights.sum(dim=-1, keepdim=True)\n",
    "        expert_weights = expert_weights.to(x.dtype)\n",
    "        expert_weights_flat = rearrange(expert_weights, '... -> (...)')\n",
    "        selected_experts_flat = rearrange(selected_experts, '... -> (...)')\n",
    "\n",
    "        bin_ids, indices, tokens_per_expert = self._sort_tokens_by_expert(selected_experts_flat)\n",
    "        padded_bins, topology = self._create_topology(x_flat, tokens_per_expert)\n",
    "        x_permuted = self._gather_tokens(x_flat, indices, bin_ids, tokens_per_expert, padded_bins)\n",
    "        x_permuted = stk.ops.sdd(x_permuted, self.w1, topology)\n",
    "        x_permuted = gelu(x_permuted)\n",
    "        x_permuted = stk.ops.dsd(x_permuted, self.w2)\n",
    "\n",
    "        x_permuted = self._scatter_tokens(x_permuted, indices, bin_ids, expert_weights_flat, tokens_per_expert, padded_bins)\n",
    "        output = rearrange(x_permuted, '(batch_size seq_len) n_embd -> batch_size seq_len n_embd', batch_size=batch_size, seq_len=seq_len)\n",
    "\n",
    "        router_z_loss = torch.logsumexp(router_logits, dim=-1).pow(2).mean()\n",
    "\n",
    "        p_i = router_probs.mean(dim=0).to(torch.bfloat16)\n",
    "\n",
    "        experts_flat = selected_experts.flatten()\n",
    "        f_i = torch.zeros(self.num_experts, dtype=x.dtype, device=x.device)\n",
    "        ones = torch.ones_like(experts_flat, dtype=x.dtype) / len(experts_flat)\n",
    "        f_i.scatter_add(0, experts_flat, ones)\n",
    "        load_balance_loss = self.num_experts * (f_i @ p_i)\n",
    "        \n",
    "        expert_assignments = rearrange(selected_experts, '(batch seq) k -> batch seq k', batch=batch_size, seq=seq_len)\n",
    "        router_logits_reshaped = rearrange(router_logits, '(batch seq) num_experts -> batch seq num_experts', batch=batch_size, seq=seq_len)\n",
    "        router_probs_reshaped = rearrange(router_probs, '(batch seq) num_experts -> batch seq num_experts', batch=batch_size, seq=seq_len)\n",
    "\n",
    "        aux_loss = {\n",
    "            'router_z_loss': router_z_loss,\n",
    "            'load_balance_loss': load_balance_loss,\n",
    "            'expert_assignments': expert_assignments,\n",
    "            'router_logits': router_logits_reshaped,\n",
    "            'router_probs': router_probs_reshaped,\n",
    "        }\n",
    "        \n",
    "        return output, aux_loss, f_i\n",
    "\n",
    "class GPTWithTracking(GPT):\n",
    "    \"\"\"Track expert assignments across layers\"\"\"\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        device = idx.device\n",
    "        b, t = idx.size()\n",
    "        assert t <= self.config.n_ctx, f\"Cannot forward sequence of length {t}, context length is only {self.config.n_ctx}\"\n",
    "        pos = torch.arange(0, t, dtype=torch.long, device=device)\n",
    "\n",
    "        tok_emb = self.transformer.wte(idx)\n",
    "        pos_emb = self.transformer.wpe(pos)\n",
    "        x = self.transformer.drop(tok_emb + pos_emb)\n",
    "        \n",
    "        combined_aux_loss = {}\n",
    "        all_expert_usage = []\n",
    "        all_expert_assignments = {}\n",
    "        all_router_logits = {}\n",
    "        all_router_probs = {}\n",
    "        aux_loss_count = 0\n",
    "\n",
    "        for layer_idx, block in enumerate(self.transformer.h):\n",
    "            block_out = block(x)\n",
    "            \n",
    "            x, aux_loss, f_i = block_out\n",
    "            \n",
    "            if f_i is not None:\n",
    "                all_expert_usage.append(f_i)\n",
    "            \n",
    "            all_expert_assignments[f'layer_{layer_idx}'] = aux_loss['expert_assignments']\n",
    "            all_router_logits[f'layer_{layer_idx}'] = aux_loss['router_logits']\n",
    "            all_router_probs[f'layer_{layer_idx}'] = aux_loss['router_probs']\n",
    "            \n",
    "            if layer_idx == 0:\n",
    "                combined_aux_loss = {k: v.clone() for k, v in aux_loss.items() \n",
    "                                if k not in ['expert_assignments', 'router_logits', 'router_probs']}\n",
    "            else:\n",
    "                for key in aux_loss:\n",
    "                    if key not in ['expert_assignments', 'router_logits', 'router_probs']:\n",
    "                        combined_aux_loss[key] += aux_loss[key]\n",
    "            \n",
    "            aux_loss_count += 1\n",
    "\n",
    "        for key in combined_aux_loss:\n",
    "            combined_aux_loss[key] /= aux_loss_count\n",
    "\n",
    "        if all_expert_usage:\n",
    "            avg_expert_usage = torch.stack(all_expert_usage).mean(dim=0)\n",
    "            combined_aux_loss['expert_usage'] = avg_expert_usage\n",
    "\n",
    "        combined_aux_loss['expert_assignments'] = all_expert_assignments\n",
    "        combined_aux_loss['router_logits'] = all_router_logits\n",
    "        combined_aux_loss['router_probs'] = all_router_probs\n",
    "\n",
    "        x = self.transformer.ln_f(x)\n",
    "        if targets is not None:\n",
    "            logits = self.lm_head(x)\n",
    "            ce_loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)\n",
    "            loss = ce_loss\n",
    "            if combined_aux_loss is not None:\n",
    "                loss = loss + self.config.load_balance_loss_weight * combined_aux_loss['load_balance_loss'] + self.config.router_z_loss_weight * combined_aux_loss['router_z_loss']\n",
    "                combined_aux_loss['ce_loss'] = ce_loss\n",
    "        else:\n",
    "            logits = self.lm_head(x[:, [-1], :])\n",
    "            loss = None\n",
    "            ce_loss = None\n",
    "\n",
    "        return logits, loss, combined_aux_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "load-model",
   "metadata": {},
   "source": [
    "## Load Model and Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = f\"{cfg['checkpoint_dir']}/ckpt.pt\"\n",
    "\n",
    "model = GPTWithTracking(config).to(torch.bfloat16)\n",
    "\n",
    "for block in model.transformer.h:\n",
    "    if hasattr(block.mlp, 'expert_sizes'):\n",
    "        old_mlp = block.mlp\n",
    "        block.mlp = MoeMLPWithTracking(config).to(torch.bfloat16)\n",
    "        block.mlp.load_state_dict(old_mlp.state_dict())\n",
    "\n",
    "checkpoint = torch.load(checkpoint_path, map_location='cpu')\n",
    "\n",
    "state_dict = checkpoint['model']\n",
    "if any(k.startswith('_orig_mod.') for k in state_dict.keys()):\n",
    "    state_dict = {k.replace('_orig_mod.', ''): v for k, v in state_dict.items()}\n",
    "\n",
    "model.load_state_dict(state_dict)\n",
    "\n",
    "print(f\"✓ Loaded checkpoint from {checkpoint_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "collect-stats",
   "metadata": {},
   "source": [
    "## Collect ALL Routing Statistics in a SINGLE Pass\n",
    "\n",
    "This cell collects all statistics (individual expert assignments and combinations) in one efficient pass through the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "single-pass",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tiktoken.get_encoding('gpt2')\n",
    "\n",
    "val_data_path = cfg['val_data_path']\n",
    "val_data = np.memmap(val_data_path, dtype=np.uint16, mode='r')\n",
    "\n",
    "device = 'cuda'\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "num_layers = config.n_layer\n",
    "expert_sizes = model.transformer.h[0].mlp.expert_sizes\n",
    "\n",
    "# Initialize ALL tracking structures\n",
    "token_stats_per_layer = {}\n",
    "token_combinations_per_layer = {}\n",
    "\n",
    "for layer_idx in range(num_layers):\n",
    "    layer_name = f'layer_{layer_idx}'\n",
    "    token_stats_per_layer[layer_name] = defaultdict(lambda: {\n",
    "        'expert_counts': np.zeros(config.num_experts, dtype=np.int64),\n",
    "        'total_occurrences': 0,\n",
    "        'total_entropy': 0.0,\n",
    "        'expert_size_sum': 0.0,\n",
    "    })\n",
    "    token_combinations_per_layer[layer_name] = defaultdict(Counter)\n",
    "\n",
    "batch_size = 1\n",
    "seq_len = 1024\n",
    "total_tokens = len(val_data)\n",
    "num_batches = total_tokens // seq_len\n",
    "\n",
    "print(f\"Running single pass through {num_batches} batches to collect all statistics...\")\n",
    "\n",
    "for batch_idx in tqdm(range(num_batches)):\n",
    "    start_idx = batch_idx * seq_len\n",
    "    end_idx = start_idx + seq_len\n",
    "    batch_tokens = torch.from_numpy(val_data[start_idx:end_idx].astype(np.int64)).unsqueeze(0).to(device)\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        logits, loss, aux_loss = model(batch_tokens, targets=batch_tokens)\n",
    "\n",
    "    output_probs = F.softmax(logits[0], dim=-1)\n",
    "    epsilon = 1e-10\n",
    "    output_entropy = -(output_probs * torch.log(output_probs + epsilon)).sum(dim=-1).float().cpu().numpy()\n",
    "\n",
    "    for layer_idx in range(num_layers):\n",
    "        layer_name = f'layer_{layer_idx}'\n",
    "        layer_assignments = aux_loss['expert_assignments'][layer_name][0].cpu().numpy()\n",
    "        token_stats = token_stats_per_layer[layer_name]\n",
    "\n",
    "        for pos in range(seq_len):\n",
    "            token_id = int(batch_tokens[0, pos].item())\n",
    "            expert_ids = layer_assignments[pos]\n",
    "\n",
    "            # Update individual expert statistics\n",
    "            token_stats[token_id]['total_occurrences'] += 1\n",
    "            token_stats[token_id]['total_entropy'] += output_entropy[pos]\n",
    "\n",
    "            for expert_id in expert_ids:\n",
    "                token_stats[token_id]['expert_counts'][expert_id] += 1\n",
    "                token_stats[token_id]['expert_size_sum'] += expert_sizes[expert_id]\n",
    "\n",
    "            # Track expert combinations\n",
    "            expert_combination = tuple(sorted(expert_ids))\n",
    "            token_combinations_per_layer[layer_name][token_id][expert_combination] += 1\n",
    "\n",
    "print(f\"\\n✓ Collected all statistics in a single pass!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "analysis",
   "metadata": {},
   "source": [
    "## Analysis: Print the Most Common Expert Combination for Each Token\n",
    "\n",
    "This shows which expert pair each token uses most frequently across all layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "print-combinations",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_expert_combo(combo, expert_sizes):\n",
    "    \"\"\"Format expert combination with sizes\"\"\"\n",
    "    return \",\".join([f\"{e}({expert_sizes[e]})\" for e in combo])\n",
    "\n",
    "def print_token_routing_table(token_combinations_per_layer, token_stats_per_layer, expert_sizes, tokenizer, max_tokens=100):\n",
    "    \"\"\"Print comprehensive routing table\"\"\"\n",
    "    \n",
    "    # Calculate average expert size and FLOPs for each token\n",
    "    token_avg_sizes = {}\n",
    "    for layer_name in token_stats_per_layer:\n",
    "        for token_id, stats in token_stats_per_layer[layer_name].items():\n",
    "            if stats['total_occurrences'] > 0:\n",
    "                avg_size = stats['expert_size_sum'] / (stats['total_occurrences'] * 2)  # 2 experts per token\n",
    "                if token_id not in token_avg_sizes:\n",
    "                    token_avg_sizes[token_id] = []\n",
    "                token_avg_sizes[token_id].append(avg_size)\n",
    "    \n",
    "    overall_avg_sizes = {tid: np.mean(sizes) for tid, sizes in token_avg_sizes.items()}\n",
    "    \n",
    "    # Header\n",
    "    print(f\"\\n{'='*200}\")\n",
    "    header = f\"{'Token ID':<10}{'Token':<20}{'Avg Size':<13}{'FLOPs':<16}\"\n",
    "    for layer_idx in range(num_layers):\n",
    "        header += f\"{'Layer ' + str(layer_idx):<21}\"\n",
    "    print(header)\n",
    "    print(f\"{'='*200}\")\n",
    "    \n",
    "    # Print first N tokens\n",
    "    for token_id in sorted(overall_avg_sizes.keys())[:max_tokens]:\n",
    "        try:\n",
    "            token_str = tokenizer.decode([token_id]).replace('\\n', '\\\\n')\n",
    "        except:\n",
    "            token_str = f\"<{token_id}>\"\n",
    "        \n",
    "        avg_size = overall_avg_sizes[token_id]\n",
    "        # FLOPs calculation: 2 * seq_len * hidden * expert_size (for matrix mult)\n",
    "        # Simplified: avg_size * hidden * 2 operations\n",
    "        flops = avg_size * 768 * 4 * 12  # hidden=768, 4x forward/backward, 12 layers\n",
    "        \n",
    "        row = f\"{token_id:<10}{token_str:<20}{avg_size:<13.1f}{flops:<16,.0f}\"\n",
    "        \n",
    "        for layer_idx in range(num_layers):\n",
    "            layer_name = f'layer_{layer_idx}'\n",
    "            if token_id in token_combinations_per_layer[layer_name]:\n",
    "                combos = token_combinations_per_layer[layer_name][token_id]\n",
    "                most_common = combos.most_common(1)[0][0]\n",
    "                combo_str = f\"({format_expert_combo(most_common, expert_sizes)})\"\n",
    "                row += f\"{combo_str:<21}\"\n",
    "            else:\n",
    "                row += f\"{'N/A':<21}\"\n",
    "        \n",
    "        print(row)\n",
    "\n",
    "print_token_routing_table(token_combinations_per_layer, token_stats_per_layer, expert_sizes, tokenizer, max_tokens=350)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a67e83c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "print(\"Building dataframe from collected statistics...\")\n",
    "\n",
    "# Get unique tokens\n",
    "all_token_ids = set()\n",
    "for layer_combos in token_combinations_per_layer.values():\n",
    "    all_token_ids.update(layer_combos.keys())\n",
    "\n",
    "# Build data for dataframe\n",
    "data = []\n",
    "for token_id in all_token_ids:\n",
    "    # Decode token\n",
    "    try:\n",
    "        token_text = tokenizer.decode([token_id])\n",
    "        token_text = token_text.replace('\\n', '\\\\n').replace('\\t', '\\\\t').replace('\\r', '\\\\r')\n",
    "        if '�' in token_text or not token_text.isprintable():\n",
    "            token_text = f\"<{token_id}>\"\n",
    "        if len(token_text) > 18:\n",
    "            token_text = token_text[:17] + '…'\n",
    "    except:\n",
    "        token_text = f\"<{token_id}>\"\n",
    "    \n",
    "    # Calculate average expert SIZE across all layers\n",
    "    total_size = 0\n",
    "    layer_count = 0\n",
    "    layer_data = {}\n",
    "    \n",
    "    for layer_idx in range(num_layers):\n",
    "        layer_name = f'layer_{layer_idx}'\n",
    "        combos = token_combinations_per_layer[layer_name][token_id]\n",
    "        if combos:\n",
    "            most_common = combos.most_common(1)[0][0]\n",
    "            layer_size = sum(expert_sizes[e] for e in most_common)\n",
    "            total_size += layer_size\n",
    "            layer_count += 1\n",
    "            # Format with expert sizes: (5(128),7(128))\n",
    "            formatted = \"(\" + \",\".join([f\"{e}({expert_sizes[e]})\" for e in most_common]) + \")\"\n",
    "            layer_data[f'layer_{layer_idx}'] = formatted\n",
    "        else:\n",
    "            layer_data[f'layer_{layer_idx}'] = 'N/A'\n",
    "    \n",
    "    avg_size = total_size / layer_count if layer_count > 0 else 0\n",
    "    flops = 4 * config.n_embd * total_size\n",
    "    \n",
    "    row = {\n",
    "        'token_id': token_id,\n",
    "        'token': token_text,\n",
    "        'avg_size': avg_size,\n",
    "        'flops': flops,\n",
    "        **layer_data\n",
    "    }\n",
    "    data.append(row)\n",
    "\n",
    "# Create DataFrame and sort by FLOPs\n",
    "df = pd.DataFrame(data)\n",
    "df = df.sort_values('flops', ascending=True).reset_index(drop=True)\n",
    "\n",
    "print(f\"\\nDataFrame created with {len(df)} tokens, sorted by FLOPs (low to high)\")\n",
    "print(f\"\\nFirst 20 rows (lowest FLOPs):\")\n",
    "print(df.head(20).to_string())\n",
    "\n",
    "print(f\"\\n\\nLast 20 rows (highest FLOPs):\")\n",
    "print(df.tail(20).to_string())\n",
    "\n",
    "# Summary statistics\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"Summary Statistics:\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"Total unique tokens: {len(df)}\")\n",
    "print(f\"\\nFLOPs distribution:\")\n",
    "print(f\"  Min:    {df['flops'].min():,.0f}\")\n",
    "print(f\"  25%:    {df['flops'].quantile(0.25):,.0f}\")\n",
    "print(f\"  Median: {df['flops'].median():,.0f}\")\n",
    "print(f\"  75%:    {df['flops'].quantile(0.75):,.0f}\")\n",
    "print(f\"  Max:    {df['flops'].max():,.0f}\")\n",
    "print(f\"  Mean:   {df['flops'].mean():,.0f}\")\n",
    "\n",
    "print(f\"\\nAverage Size distribution:\")\n",
    "print(f\"  Tokens with avg_size >= 2560: {(df['avg_size'] >= 2560).sum()} ({100*(df['avg_size'] >= 2560).sum()/len(df):.2f}%)\")\n",
    "print(f\"  Tokens with avg_size < 2560:  {(df['avg_size'] < 2560).sum()} ({100*(df['avg_size'] < 2560).sum()/len(df):.2f}%)\")\n",
    "\n",
    "baseline_flops = 8 * 4 * 640 * 2560  # num_layers * 4 * hidden_size * expert_size\n",
    "print(f\"\\nAverage FLOPs per token: {df['flops'].mean():.0f} ({100*df['flops'].mean()/baseline_flops:.2f}% of baseline)\")\n",
    "\n",
    "# Store the dataframe for further analysis\n",
    "expert_combinations_df = df\n",
    "sweep_value = '-'.join(checkpoint_path.split('-')).split('/')[-2]\n",
    "\n",
    "df.to_csv(f'analysis_csvs/{sweep_value}_expert_combinations.csv', index=False)\n",
    "print(f\"\\n✓ Saved to analysis_csvs/{sweep_value}_expert_combinations.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc0c44bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze per-token statistics FOR EACH LAYER\n",
    "for layer_idx in range(num_layers):\n",
    "    layer_name = f'layer_{layer_idx}'\n",
    "    token_stats = token_stats_per_layer[layer_name]\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"LAYER {layer_idx} ANALYSIS\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    # Compute derived metrics for each token\n",
    "    token_analysis = {}\n",
    "    \n",
    "    for token_id, stats in token_stats.items():\n",
    "        if stats['total_occurrences'] > 0:\n",
    "            # Average entropy\n",
    "            avg_entropy = stats['total_entropy'] / stats['total_occurrences']\n",
    "            \n",
    "            # Expert distribution (normalized)\n",
    "            expert_distribution = stats['expert_counts'] / stats['expert_counts'].sum()\n",
    "            \n",
    "            # Most common expert\n",
    "            most_common_expert = np.argmax(stats['expert_counts'])\n",
    "            \n",
    "            # Average expert size\n",
    "            avg_expert_size = stats['expert_size_sum'] / stats['expert_counts'].sum()\n",
    "            \n",
    "            token_analysis[token_id] = {\n",
    "                'avg_entropy': avg_entropy,\n",
    "                'occurrences': stats['total_occurrences'],\n",
    "                'expert_distribution': expert_distribution,\n",
    "                'most_common_expert': most_common_expert,\n",
    "                'avg_expert_size': avg_expert_size,\n",
    "            }\n",
    "    \n",
    "    # Plot distribution of average expert sizes\n",
    "    all_expert_sizes = np.array([a['avg_expert_size'] for a in token_analysis.values()])\n",
    "    all_occurrences = np.array([a['occurrences'] for a in token_analysis.values()])\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "    fig.suptitle(f'Layer {layer_idx} Token Routing Analysis', fontsize=16)\n",
    "    \n",
    "    # Unweighted histogram (by unique tokens)\n",
    "    ax1 = axes[0]\n",
    "    ax1.hist(all_expert_sizes, bins=50, alpha=0.7, edgecolor='black')\n",
    "    ax1.axvline(x=128, color='blue', linestyle='--', label='Small (128)', linewidth=2)\n",
    "    ax1.axvline(x=2944, color='red', linestyle='--', label='Large (2944)', linewidth=2)\n",
    "    ax1.axvline(x=np.mean(all_expert_sizes), color='green', linestyle='--', label=f'Mean ({np.mean(all_expert_sizes):.0f})', linewidth=2)\n",
    "    ax1.set_xlabel('Average Expert Size per Token')\n",
    "    ax1.set_ylabel('Number of Unique Tokens')\n",
    "    ax1.set_title('Distribution by Unique Tokens (Unweighted)')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Weighted histogram (by token occurrences)\n",
    "    ax2 = axes[1]\n",
    "    ax2.hist(all_expert_sizes, bins=50, weights=all_occurrences, alpha=0.7, edgecolor='black', color='orange')\n",
    "    ax2.axvline(x=128, color='blue', linestyle='--', label='Small (128)', linewidth=2)\n",
    "    ax2.axvline(x=2944, color='red', linestyle='--', label='Large (2944)', linewidth=2)\n",
    "    weighted_mean = np.average(all_expert_sizes, weights=all_occurrences)\n",
    "    ax2.axvline(x=weighted_mean, color='green', linestyle='--', label=f'Weighted Mean ({weighted_mean:.0f})', linewidth=2)\n",
    "    ax2.set_xlabel('Average Expert Size per Token')\n",
    "    ax2.set_ylabel('Total Token Occurrences')\n",
    "    ax2.set_title('Distribution by Token Occurrences (Weighted)')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Pie chart - weighted breakdown\n",
    "    ax3 = axes[2]\n",
    "    large_expert_occurrences = sum(all_occurrences[all_expert_sizes > 1536])\n",
    "    small_expert_occurrences = sum(all_occurrences[all_expert_sizes <= 1536])\n",
    "    total_occurrences = large_expert_occurrences + small_expert_occurrences\n",
    "    \n",
    "    ax3.pie([large_expert_occurrences, small_expert_occurrences],\n",
    "            labels=['Large experts\\n(>1536)', 'Small experts\\n(≤1536)'],\n",
    "            autopct='%1.1f%%',\n",
    "            colors=['red', 'blue'])\n",
    "    ax3.set_title(f'Token Occurrences by Expert Size\\n(Total: {total_occurrences:,} tokens)')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Statistics\n",
    "    print(f\"Average Expert Size Statistics (Unweighted):\")\n",
    "    print(f\"  Mean: {np.mean(all_expert_sizes):.2f}\")\n",
    "    print(f\"  Median: {np.median(all_expert_sizes):.2f}\")\n",
    "    print(f\"  Std: {np.std(all_expert_sizes):.2f}\")\n",
    "    \n",
    "    print(f\"\\nAverage Expert Size Statistics (Weighted by occurrences):\")\n",
    "    print(f\"  Weighted Mean: {weighted_mean:.2f}\")\n",
    "    \n",
    "    # Count how many tokens go to mostly large vs mostly small experts\n",
    "    large_expert_tokens = sum(1 for s in all_expert_sizes if s > 1536)\n",
    "    small_expert_tokens = sum(1 for s in all_expert_sizes if s <= 1536)\n",
    "    print(f\"\\nUnique Token routing breakdown:\")\n",
    "    print(f\"  Unique tokens routing mostly to LARGE experts: {large_expert_tokens} ({100*large_expert_tokens/len(all_expert_sizes):.1f}%)\")\n",
    "    print(f\"  Unique tokens routing mostly to SMALL experts: {small_expert_tokens} ({100*small_expert_tokens/len(all_expert_sizes):.1f}%)\")\n",
    "    \n",
    "    print(f\"\\nWeighted by occurrences:\")\n",
    "    print(f\"  Token occurrences routed to LARGE experts: {large_expert_occurrences:,} ({100*large_expert_occurrences/total_occurrences:.1f}%)\")\n",
    "    print(f\"  Token occurrences routed to SMALL experts: {small_expert_occurrences:,} ({100*small_expert_occurrences/total_occurrences:.1f}%)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SUMMARY ACROSS ALL LAYERS\")\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
