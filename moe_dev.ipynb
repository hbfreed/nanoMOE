{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from dataclasses import dataclass\n",
    "from einops import rearrange\n",
    "from moe import sdd_kernel\n",
    "\n",
    "@dataclass\n",
    "class ToyMoEConfig:\n",
    "    n_embd: int = 768\n",
    "    num_experts: int = 8\n",
    "    num_experts_per_tok: int = 2\n",
    "    norm_topk_prob: bool = True\n",
    "    bias: bool = True\n",
    "    dropout: float = 0.0\n",
    "\n",
    "class MoeMLP(nn.Module):\n",
    "    def __init__(self, config,\n",
    "                 BLOCK_M: int = 128,\n",
    "                 BLOCK_N: int = 128,\n",
    "                 BLOCK_K: int = 32):\n",
    "        super().__init__()\n",
    "        self.num_experts = config.num_experts\n",
    "        self.num_experts_per_tok = config.num_experts_per_tok  # \"k\"\n",
    "        self.norm_topk_prob = config.norm_topk_prob\n",
    "        self.n_embd = config.n_embd\n",
    "\n",
    "        d_ffn = (4 * self.n_embd) // self.num_experts_per_tok\n",
    "        self.d_ffn = ((d_ffn + BLOCK_N - 1) // BLOCK_N) * BLOCK_N  # round up for kernel friendliness\n",
    "\n",
    "        # Router\n",
    "        self.router = nn.Linear(self.n_embd, self.num_experts, bias=False)\n",
    "\n",
    "        # Expert matrices packed together\n",
    "        self.w1 = nn.Parameter(torch.empty(self.n_embd, self.d_ffn * self.num_experts))\n",
    "        self.w2 = nn.Parameter(torch.empty(self.d_ffn * self.num_experts, self.n_embd))\n",
    "        nn.init.xavier_uniform_(self.w1)\n",
    "        nn.init.xavier_uniform_(self.w2)\n",
    "\n",
    "        self.BLOCK_M = BLOCK_M\n",
    "        self.BLOCK_N = BLOCK_N\n",
    "        self.BLOCK_K = BLOCK_K\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, n_embd = x.shape\n",
    "        num_tokens = batch_size * seq_len\n",
    "\n",
    "        x_flat = rearrange(x, 'batch seq hidden -> (batch seq) hidden') #flatten to just tokens by hidden size\n",
    "        assert x_flat.data_ptr() == x.data_ptr()  # sanity check that we're just doing a view\n",
    "\n",
    "        router_logits = self.router(x_flat)\n",
    "        router_weights = F.softmax(router_logits, dim=1, dtype=torch.float) #float32 here for stability\n",
    "        router_weights, selected_experts = torch.topk(router_weights, self.num_experts_per_tok, dim=-1)\n",
    "\n",
    "        if self.norm_topk_prob:\n",
    "            router_weights /= router_weights.sum(dim=-1, keepdim=True) #normalize to 1\n",
    "        router_weights = router_weights.to(x.dtype)\n",
    "        selected_experts = selected_experts.to(torch.int32)\n",
    "\n",
    "        x_rep = x_flat.repeat_interleave(self.num_experts_per_tok, dim=0) #make k token copies to map to the experts\n",
    "        selected_experts_rep = selected_experts.reshape(-1)\n",
    "        router_weights_rep = router_weights.reshape(-1, 1)\n",
    "\n",
    "        perm = torch.argsort(selected_experts_rep, stable=True)\n",
    "        x_grouped = x_rep[perm]\n",
    "        selected_experts_sorted = selected_experts_rep[perm]\n",
    "        router_weights_sorted = router_weights_rep[perm]\n",
    "\n",
    "        inv_perm = torch.empty_like(perm)\n",
    "        inv_perm[perm] = torch.arange(perm.numel(), device=x.device)\n",
    "\n",
    "        row_indices_ptr = torch.div(\n",
    "            torch.arange(x_grouped.size(0), device=x.device, dtype=torch.int32),\n",
    "            self.BLOCK_M, rounding_mode='floor'\n",
    "        )\n",
    "        block_mask = ((torch.arange(x_grouped.size(0), device=x.device) % self.BLOCK_M) == 0)\n",
    "        row_indices_ptr = row_indices_ptr[block_mask].contiguous()\n",
    "        col_indices_ptr = selected_experts_sorted[block_mask].contiguous()\n",
    "\n",
    "        block_sparse = torch.empty(x_grouped.size(0), self.d_ffn, dtype=x.dtype, device=x.device)\n",
    "\n",
    "        stride_xm, stride_xk = x_grouped.stride()\n",
    "        stride_om, stride_on = block_sparse.stride()\n",
    "        stride_wk, stride_wn = self.w1.stride()\n",
    "\n",
    "        num_active_blocks = row_indices_ptr.size(0)\n",
    "        grid = (num_active_blocks,)\n",
    "        \n",
    "        # DEBUG: Validate kernel parameters before calling\n",
    "        print(f\"\\nüîß KERNEL DEBUG INFO:\")\n",
    "        print(f\"  x_grouped.shape: {x_grouped.shape}\")\n",
    "        print(f\"  w1.shape: {self.w1.shape}\")  \n",
    "        print(f\"  block_sparse.shape: {block_sparse.shape}\")\n",
    "        print(f\"  row_indices_ptr: {row_indices_ptr}\")\n",
    "        print(f\"  col_indices_ptr: {col_indices_ptr}\")\n",
    "        print(f\"  M={x_grouped.size(0)}, N={self.d_ffn}, K={self.n_embd}\")\n",
    "        print(f\"  num_active_blocks: {num_active_blocks}\")\n",
    "        print(f\"  d_ffn calculation: (4 * {self.n_embd}) // {self.num_experts_per_tok} = {(4 * self.n_embd) // self.num_experts_per_tok}\")\n",
    "        print(f\"  d_ffn rounded: {self.d_ffn}\")\n",
    "        \n",
    "        # Check critical bounds\n",
    "        if row_indices_ptr.numel() > 0:\n",
    "            print(f\"  row_indices range: [{row_indices_ptr.min()}, {row_indices_ptr.max()}] (should be < {x_grouped.size(0)})\")\n",
    "            if row_indices_ptr.max() >= x_grouped.size(0):\n",
    "                print(f\"  ‚ùå ERROR: row indices out of bounds!\")\n",
    "                \n",
    "        if col_indices_ptr.numel() > 0:\n",
    "            print(f\"  col_indices range: [{col_indices_ptr.min()}, {col_indices_ptr.max()}] (should be < {self.num_experts})\")\n",
    "            if col_indices_ptr.max() >= self.num_experts:\n",
    "                print(f\"  ‚ùå ERROR: col indices out of bounds!\")\n",
    "                \n",
    "        # Check weight matrix access bounds\n",
    "        expected_w1_cols = self.d_ffn * self.num_experts\n",
    "        print(f\"  w1 expected cols: {expected_w1_cols}, actual: {self.w1.shape[1]}\")\n",
    "        if self.w1.shape[1] != expected_w1_cols:\n",
    "            print(f\"  ‚ùå ERROR: w1 matrix dimension mismatch!\")\n",
    "\n",
    "        sdd_kernel[grid](\n",
    "            x_ptr=x_grouped,\n",
    "            w1_ptr=self.w1,\n",
    "            output_ptr=block_sparse,\n",
    "            row_indices_ptr=row_indices_ptr,\n",
    "            col_indices_ptr=col_indices_ptr,\n",
    "            M=x_grouped.size(0),\n",
    "            N=self.d_ffn,\n",
    "            K=self.n_embd,\n",
    "            stride_xm=stride_xm, stride_xk=stride_xk,\n",
    "            stride_wk=stride_wk, stride_wn=stride_wn,\n",
    "            stride_om=stride_om, stride_on=stride_on,\n",
    "            BLOCK_M=self.BLOCK_M, BLOCK_N=self.BLOCK_N, BLOCK_K=self.BLOCK_K\n",
    "        )\n",
    "        \n",
    "        # Apply GELU activation (this would normally be part of the kernel)\n",
    "        block_sparse = F.gelu(block_sparse)\n",
    "        \n",
    "        # Apply router weights\n",
    "        block_sparse = block_sparse * router_weights_sorted\n",
    "        \n",
    "        print(f\"Block sparse output shape: {block_sparse.shape}\")\n",
    "        print(f\"First few values: {block_sparse[:3, :5]}\")\n",
    "        \n",
    "        return block_sparse, router_logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config: ToyMoEConfig(n_embd=768, num_experts=8, num_experts_per_tok=2, norm_topk_prob=True, bias=True, dropout=0.0)\n",
      "\n",
      "Input shape: torch.Size([4, 8, 768])\n",
      "\n",
      "MoeMLP d_ffn: 1536\n",
      "w1 shape: torch.Size([768, 12288])\n",
      "w2 shape: torch.Size([12288, 768])\n",
      "\n",
      "=== Forward Pass ===\n",
      "\n",
      "üîß KERNEL DEBUG INFO:\n",
      "  x_grouped.shape: torch.Size([64, 768])\n",
      "  w1.shape: torch.Size([768, 12288])\n",
      "  block_sparse.shape: torch.Size([64, 1536])\n",
      "  row_indices_ptr: tensor([0], device='cuda:0', dtype=torch.int32)\n",
      "  col_indices_ptr: tensor([0], device='cuda:0', dtype=torch.int32)\n",
      "  M=64, N=1536, K=768\n",
      "  num_active_blocks: 1\n",
      "  d_ffn calculation: (4 * 768) // 2 = 1536\n",
      "  d_ffn rounded: 1536\n",
      "  row_indices range: [0, 0] (should be < 64)\n",
      "  col_indices range: [0, 0] (should be < 8)\n",
      "  w1 expected cols: 12288, actual: 12288\n",
      "Block sparse output shape: torch.Size([64, 1536])\n",
      "First few values: tensor([[-0.0808, -0.0615,  0.2778,  0.3480,  0.0026],\n",
      "        [ 0.0179,  0.0020,  0.0027, -0.0417,  0.0513],\n",
      "        [-0.0086,  0.2000, -0.0539, -0.0605, -0.0525]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "\n",
      "Forward pass completed!\n",
      "Router logits shape: torch.Size([32, 8])\n",
      "Output shape: torch.Size([64, 1536])\n",
      "\n",
      "Router logits sample:\n",
      "tensor([[ 1.2212,  0.0832, -0.8844, -0.1742],\n",
      "        [-0.1503, -0.8337, -0.6974, -0.3472],\n",
      "        [ 0.3110,  0.5651,  0.8854,  0.6571]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Output sample:\n",
      "tensor([[-0.0808, -0.0615,  0.2778],\n",
      "        [ 0.0179,  0.0020,  0.0027],\n",
      "        [-0.0086,  0.2000, -0.0539],\n",
      "        [ 0.0617, -0.1153, -0.0961],\n",
      "        [ 0.1901,  0.0789, -0.0022]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Test the MoeMLP\n",
    "config = ToyMoEConfig()\n",
    "print(f\"Config: {config}\")\n",
    "\n",
    "# Create toy input\n",
    "batch_size = 4\n",
    "seq_len = 8\n",
    "x = torch.randn(batch_size, seq_len, config.n_embd,device='cuda')\n",
    "print(f\"\\nInput shape: {x.shape}\")\n",
    "\n",
    "# Create MoeMLP instance\n",
    "moe_mlp = MoeMLP(config).cuda()\n",
    "print(f\"\\nMoeMLP d_ffn: {moe_mlp.d_ffn}\")\n",
    "print(f\"w1 shape: {moe_mlp.w1.shape}\")\n",
    "print(f\"w2 shape: {moe_mlp.w2.shape}\")\n",
    "\n",
    "# Test forward pass\n",
    "print(f\"\\n=== Forward Pass ===\")\n",
    "output, router_logits = moe_mlp(x)\n",
    "print(f\"\\nForward pass completed!\")\n",
    "print(f\"Router logits shape: {router_logits.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "\n",
    "# Show some debug info\n",
    "print(f\"\\nRouter logits sample:\\n{router_logits[:3, :4]}\")\n",
    "print(f\"Output sample:\\n{output[:5, :3]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the traditional MLP and MoeMLPForLoop from model.py for comparison\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd, bias=config.bias)\n",
    "        self.gelu    = nn.GELU()\n",
    "        self.c_proj  = nn.Linear(4 * config.n_embd, config.n_embd, bias=config.bias)\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "\n",
    "    def forward(self, x, first_matmul_only=False):\n",
    "        x = self.c_fc(x)\n",
    "        x = self.gelu(x)\n",
    "        if first_matmul_only:\n",
    "            return x  # Return after first matmul + GELU, like MoeMLP\n",
    "        x = self.c_proj(x)\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "\n",
    "class MoeMLPForLoop(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.num_experts = config.num_experts\n",
    "        self.num_experts_per_tok = config.num_experts_per_tok #top k\n",
    "        self.norm_topk_prob = config.norm_topk_prob #bool, normalize the topk probabilities, or not?\n",
    "\n",
    "        self.router = nn.Linear(config.n_embd, self.num_experts, bias=False)\n",
    "\n",
    "        self.experts = nn.ModuleList([\n",
    "            MLP(config) for _ in range(self.num_experts)            \n",
    "        ])\n",
    "\n",
    "    def forward(self, x, first_matmul_only=False):\n",
    "        batch_size, seq_len, hidden_dim = x.shape\n",
    "        x_flat = x.view(-1, hidden_dim)\n",
    "\n",
    "        router_logits = self.router(x_flat)\n",
    "        router_weights = F.softmax(router_logits, dim=1, dtype=torch.float) #float32 here for stability\n",
    "        router_weights, selected_experts = torch.topk(router_weights, self.num_experts_per_tok, dim=-1)\n",
    "\n",
    "        if self.norm_topk_prob:\n",
    "            router_weights /= router_weights.sum(dim=-1, keepdim=True) #normalize to 1 if we have normalization on\n",
    "        router_weights = router_weights.to(x.dtype)\n",
    "\n",
    "        # Determine output size based on first_matmul_only\n",
    "        if first_matmul_only:\n",
    "            output_size = 4 * hidden_dim  # After first matmul: n_embd -> 4*n_embd\n",
    "            # Create output to collect intermediate results (sorted like MoeMLP)\n",
    "            output = torch.zeros(x_flat.size(0) * self.num_experts_per_tok, output_size, \n",
    "                               dtype=x.dtype, device=x.device)\n",
    "            token_indices = []\n",
    "            expert_indices = []\n",
    "        else:\n",
    "            output = torch.zeros_like(x_flat)\n",
    "\n",
    "        expert_mask = F.one_hot(selected_experts, num_classes=self.num_experts) #keep track which experts are active\n",
    "        \n",
    "        # n = batch * seq_len (number of tokens), k = num_experts_per_tok/ e = num_experts\n",
    "        expert_mask = rearrange(expert_mask, 'n k e -> e k n')\n",
    "\n",
    "        output_idx = 0\n",
    "        for expert_idx in range(self.num_experts):\n",
    "            idx, top_x = torch.where(expert_mask[expert_idx])\n",
    "\n",
    "            if len(top_x) > 0:\n",
    "                current_state = x_flat[top_x]\n",
    "                current_output = self.experts[expert_idx](current_state, first_matmul_only=first_matmul_only)\n",
    "                \n",
    "                if first_matmul_only:\n",
    "                    # Store intermediate results with router weights applied\n",
    "                    weighted_output = current_output * router_weights[top_x, idx, None]\n",
    "                    output[output_idx:output_idx+len(top_x)] = weighted_output\n",
    "                    token_indices.extend(top_x.tolist())\n",
    "                    expert_indices.extend([expert_idx] * len(top_x))\n",
    "                    output_idx += len(top_x)\n",
    "                else:\n",
    "                    weighted_output = current_output * router_weights[top_x, idx, None]\n",
    "                    output.index_add_(0, top_x, weighted_output.to(x.dtype))\n",
    "        \n",
    "        if first_matmul_only:\n",
    "            # Trim output to actual size and return metadata\n",
    "            output = output[:output_idx]\n",
    "            return output, router_logits, torch.tensor(token_indices), torch.tensor(expert_indices)\n",
    "        else:\n",
    "            return output.view(batch_size, seq_len, hidden_dim), router_logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== COMPARISON: MoeMLP vs MoeMLPForLoop ===\n",
      "\n",
      "Input shape: torch.Size([4, 8, 768])\n",
      "Config: 8 experts, 2 active per token\n",
      "\n",
      "1. MoeMLPForLoop (traditional for-loop) - first matmul only:\n",
      "   Parameters: 37,785,600\n",
      "   Output shape: torch.Size([64, 3072])\n",
      "   Router logits shape: torch.Size([32, 8])\n",
      "   Forward time: 0.0196s\n",
      "   Output sample:\n",
      "tensor([[ 0.1278,  0.1584, -0.0163, -0.0498, -0.0735],\n",
      "        [-0.0165, -0.0927,  0.0483, -0.0364,  0.0823],\n",
      "        [ 0.6539, -0.0609, -0.0853,  0.3039,  0.2229]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "   Token indices: tensor([ 0, 22, 24, 10, 12, 18, 23,  2, 10, 18])\n",
      "   Expert indices: tensor([0, 0, 0, 0, 0, 0, 0, 1, 1, 1])\n",
      "\n",
      "2. MoeMLP (kernel approach - incomplete):\n",
      "   Parameters: 18,880,512\n",
      "\n",
      "üîß KERNEL DEBUG INFO:\n",
      "  x_grouped.shape: torch.Size([64, 768])\n",
      "  w1.shape: torch.Size([768, 12288])\n",
      "  block_sparse.shape: torch.Size([64, 1536])\n",
      "  row_indices_ptr: tensor([0], device='cuda:0', dtype=torch.int32)\n",
      "  col_indices_ptr: tensor([0], device='cuda:0', dtype=torch.int32)\n",
      "  M=64, N=1536, K=768\n",
      "  num_active_blocks: 1\n",
      "  d_ffn calculation: (4 * 768) // 2 = 1536\n",
      "  d_ffn rounded: 1536\n",
      "  row_indices range: [0, 0] (should be < 64)\n",
      "  col_indices range: [0, 0] (should be < 8)\n",
      "  w1 expected cols: 12288, actual: 12288\n",
      "Block sparse output shape: torch.Size([64, 1536])\n",
      "First few values: tensor([[ 0.0130,  0.0851, -0.0627,  0.1187, -0.0202],\n",
      "        [-0.1108,  0.1087,  0.1277,  0.0226,  0.1797],\n",
      "        [-0.0511, -0.0599, -0.0241, -0.0299, -0.0082]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "   Output shape: torch.Size([64, 1536])\n",
      "   Router logits shape: torch.Size([32, 8])\n",
      "   Forward time: 0.0026s\n",
      "   Output sample:\n",
      "tensor([[ 0.0130,  0.0851, -0.0627,  0.1187, -0.0202],\n",
      "        [-0.1108,  0.1087,  0.1277,  0.0226,  0.1797],\n",
      "        [-0.0511, -0.0599, -0.0241, -0.0299, -0.0082],\n",
      "        [-0.0363, -0.0222, -0.0717, -0.0683,  0.0243],\n",
      "        [-0.0304, -0.0338, -0.0122, -0.0208, -0.0463]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "\n",
      "3. Router Comparison:\n",
      "   Router logits difference (should be different due to different router weights): 0.626306\n",
      "\n",
      "4. Key Differences (both doing first matmul only now):\n",
      "   ‚Ä¢ MoeMLPForLoop: Returns intermediate shape torch.Size([64, 3072])\n",
      "   ‚Ä¢ MoeMLP: Returns intermediate shape torch.Size([64, 1536])\n",
      "   ‚Ä¢ MoeMLPForLoop: 8 separate MLP modules\n",
      "   ‚Ä¢ MoeMLP: Single packed weight matrices\n",
      "   ‚Ä¢ MoeMLPForLoop: For-loop over experts (inefficient)\n",
      "   ‚Ä¢ MoeMLP: Triton kernel for sparse computation (efficient)\n",
      "   ‚Ä¢ MoeMLPForLoop: Natural token -> expert ordering\n",
      "   ‚Ä¢ MoeMLP: Sorted by expert for kernel efficiency\n",
      "\n",
      "5. Next Steps:\n",
      "   ‚Ä¢ Compare intermediate outputs to verify correctness\n",
      "   ‚Ä¢ Add second matmul (w2) to complete MoeMLP\n",
      "   ‚Ä¢ Add inverse permutation to restore token order\n",
      "   ‚Ä¢ Add proper loss/balancing terms\n",
      "\n",
      "‚úÖ Comparison complete!\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# Compare MoeMLP vs MoeMLPForLoop\n",
    "print(\"=== COMPARISON: MoeMLP vs MoeMLPForLoop ===\\n\")\n",
    "\n",
    "# Use the same input for both\n",
    "torch.manual_seed(42)  # For reproducible comparison\n",
    "x_test = torch.randn(batch_size, seq_len, config.n_embd, device='cuda')\n",
    "\n",
    "print(f\"Input shape: {x_test.shape}\")\n",
    "print(f\"Config: {config.num_experts} experts, {config.num_experts_per_tok} active per token\")\n",
    "\n",
    "# Test MoeMLPForLoop (traditional approach) - first matmul only for fair comparison\n",
    "print(f\"\\n1. MoeMLPForLoop (traditional for-loop) - first matmul only:\")\n",
    "moe_forloop = MoeMLPForLoop(config).cuda()\n",
    "\n",
    "# Count parameters\n",
    "forloop_params = sum(p.numel() for p in moe_forloop.parameters())\n",
    "print(f\"   Parameters: {forloop_params:,}\")\n",
    "\n",
    "start_time = time.time()\n",
    "forloop_output, forloop_router_logits, forloop_token_indices, forloop_expert_indices = moe_forloop(x_test, first_matmul_only=True)\n",
    "forloop_time = time.time() - start_time\n",
    "\n",
    "print(f\"   Output shape: {forloop_output.shape}\")\n",
    "print(f\"   Router logits shape: {forloop_router_logits.shape}\")\n",
    "print(f\"   Forward time: {forloop_time:.4f}s\")\n",
    "print(f\"   Output sample:\\n{forloop_output[:3, :5]}\")\n",
    "print(f\"   Token indices: {forloop_token_indices[:10]}\")  # First 10 token assignments\n",
    "print(f\"   Expert indices: {forloop_expert_indices[:10]}\")  # First 10 expert assignments\n",
    "\n",
    "# Test MoeMLP (kernel approach - but incomplete)\n",
    "print(f\"\\n2. MoeMLP (kernel approach - incomplete):\")\n",
    "moe_kernel = MoeMLP(config).cuda()\n",
    "\n",
    "kernel_params = sum(p.numel() for p in moe_kernel.parameters())\n",
    "print(f\"   Parameters: {kernel_params:,}\")\n",
    "\n",
    "start_time = time.time()\n",
    "kernel_output, kernel_router_logits = moe_kernel(x_test)\n",
    "kernel_time = time.time() - start_time\n",
    "\n",
    "print(f\"   Output shape: {kernel_output.shape}\")\n",
    "print(f\"   Router logits shape: {kernel_router_logits.shape}\")\n",
    "print(f\"   Forward time: {kernel_time:.4f}s\")\n",
    "print(f\"   Output sample:\\n{kernel_output[:5, :5]}\")\n",
    "\n",
    "# Compare router behavior\n",
    "print(f\"\\n3. Router Comparison:\")\n",
    "print(f\"   Router logits difference (should be different due to different router weights): {torch.abs(forloop_router_logits - kernel_router_logits).mean():.6f}\")\n",
    "\n",
    "# Key differences\n",
    "print(f\"\\n4. Key Differences (both doing first matmul only now):\")\n",
    "print(f\"   ‚Ä¢ MoeMLPForLoop: Returns intermediate shape {forloop_output.shape}\")\n",
    "print(f\"   ‚Ä¢ MoeMLP: Returns intermediate shape {kernel_output.shape}\")\n",
    "print(f\"   ‚Ä¢ MoeMLPForLoop: {config.num_experts} separate MLP modules\")\n",
    "print(f\"   ‚Ä¢ MoeMLP: Single packed weight matrices\")\n",
    "print(f\"   ‚Ä¢ MoeMLPForLoop: For-loop over experts (inefficient)\")\n",
    "print(f\"   ‚Ä¢ MoeMLP: Triton kernel for sparse computation (efficient)\")\n",
    "print(f\"   ‚Ä¢ MoeMLPForLoop: Natural token -> expert ordering\")\n",
    "print(f\"   ‚Ä¢ MoeMLP: Sorted by expert for kernel efficiency\")\n",
    "\n",
    "print(f\"\\n5. Next Steps:\")\n",
    "print(f\"   ‚Ä¢ Compare intermediate outputs to verify correctness\")\n",
    "print(f\"   ‚Ä¢ Add second matmul (w2) to complete MoeMLP\")\n",
    "print(f\"   ‚Ä¢ Add inverse permutation to restore token order\")\n",
    "print(f\"   ‚Ä¢ Add proper loss/balancing terms\")\n",
    "\n",
    "print(f\"\\n‚úÖ Comparison complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== DETAILED ROUTING ANALYSIS ===\n",
      "\n",
      "‚úÖ Synchronized router weights between models\n",
      "\n",
      "Input shape: torch.Size([2, 4, 768])\n",
      "\n",
      "üîß KERNEL DEBUG INFO:\n",
      "  x_grouped.shape: torch.Size([16, 768])\n",
      "  w1.shape: torch.Size([768, 12288])\n",
      "  block_sparse.shape: torch.Size([16, 1536])\n",
      "  row_indices_ptr: tensor([0], device='cuda:0', dtype=torch.int32)\n",
      "  col_indices_ptr: tensor([0], device='cuda:0', dtype=torch.int32)\n",
      "  M=16, N=1536, K=768\n",
      "  num_active_blocks: 1\n",
      "  d_ffn calculation: (4 * 768) // 2 = 1536\n",
      "  d_ffn rounded: 1536\n",
      "  row_indices range: [0, 0] (should be < 16)\n",
      "  col_indices range: [0, 0] (should be < 8)\n",
      "  w1 expected cols: 12288, actual: 12288\n",
      "Block sparse output shape: torch.Size([16, 1536])\n",
      "First few values: tensor([[-0.0328,  0.1307, -0.0284, -0.0225,  0.2697],\n",
      "        [ 0.0213,  0.1400, -0.1031,  0.1953, -0.0333],\n",
      "        [-0.0422,  0.0414,  0.0487,  0.0086,  0.0685]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "\n",
      "Router logits difference (should be ~0 now): 0.00000000\n",
      "\n",
      "Routing Analysis for 8 tokens:\n",
      "Selected experts per token:\n",
      "  Token 0: experts [0, 7] with weights ['0.505', '0.495']\n",
      "  Token 1: experts [4, 6] with weights ['0.687', '0.313']\n",
      "  Token 2: experts [1, 6] with weights ['0.673', '0.327']\n",
      "  Token 3: experts [5, 3] with weights ['0.540', '0.460']\n",
      "  Token 4: experts [7, 2] with weights ['0.740', '0.260']\n",
      "  Token 5: experts [3, 2] with weights ['0.544', '0.456']\n",
      "  Token 6: experts [5, 4] with weights ['0.551', '0.449']\n",
      "  Token 7: experts [7, 5] with weights ['0.679', '0.321']\n",
      "\n",
      "Expert usage distribution:\n",
      "  Expert 0: used 1 times (6.2%)\n",
      "  Expert 1: used 1 times (6.2%)\n",
      "  Expert 2: used 2 times (12.5%)\n",
      "  Expert 3: used 2 times (12.5%)\n",
      "  Expert 4: used 2 times (12.5%)\n",
      "  Expert 5: used 3 times (18.8%)\n",
      "  Expert 6: used 2 times (12.5%)\n",
      "  Expert 7: used 3 times (18.8%)\n",
      "\n",
      "Load balancing (lower is better): 0.76\n",
      "Perfect balance would be: 2.0 uses per expert\n",
      "\n",
      "=== MEMORY USAGE ===\n",
      "MoeMLPForLoop memory: 144.14 MB\n",
      "MoeMLP memory: 72.02 MB\n",
      "Memory ratio (kernel/forloop): 0.50x\n",
      "\n",
      "=== PERFORMANCE POTENTIAL ===\n",
      "‚Ä¢ MoeMLPForLoop must process ALL experts sequentially\n",
      "‚Ä¢ MoeMLP only processes ACTIVE expert blocks in parallel\n",
      "‚Ä¢ With 2/8 experts active, theoretical speedup: 4.0x\n",
      "‚Ä¢ Plus Triton kernel optimization for memory efficiency\n",
      "\n",
      "‚úÖ Analysis complete!\n"
     ]
    }
   ],
   "source": [
    "# Detailed analysis of routing behavior\n",
    "print(\"=== DETAILED ROUTING ANALYSIS ===\\n\")\n",
    "\n",
    "# Let's set both models to use the same router weights for fair comparison\n",
    "with torch.no_grad():\n",
    "    # Copy router weights from forloop to kernel model\n",
    "    moe_kernel.router.weight.copy_(moe_forloop.router.weight)\n",
    "\n",
    "print(\"‚úÖ Synchronized router weights between models\")\n",
    "\n",
    "# Now test with same routing\n",
    "torch.manual_seed(42)\n",
    "x_sync = torch.randn(2, 4, config.n_embd, device='cuda')  # Smaller for easier analysis\n",
    "\n",
    "print(f\"\\nInput shape: {x_sync.shape}\")\n",
    "\n",
    "# Get routing decisions from both models (first matmul only for fair comparison)\n",
    "forloop_output2, forloop_logits2, forloop_tokens2, forloop_experts2 = moe_forloop(x_sync, first_matmul_only=True)\n",
    "kernel_output2, kernel_logits2 = moe_kernel(x_sync)\n",
    "\n",
    "print(f\"\\nRouter logits difference (should be ~0 now): {torch.abs(forloop_logits2 - kernel_logits2).max():.8f}\")\n",
    "\n",
    "# Analyze routing decisions\n",
    "router_weights = F.softmax(forloop_logits2, dim=1, dtype=torch.float)\n",
    "router_weights, selected_experts = torch.topk(router_weights, config.num_experts_per_tok, dim=-1)\n",
    "\n",
    "if config.norm_topk_prob:\n",
    "    router_weights /= router_weights.sum(dim=-1, keepdim=True)\n",
    "\n",
    "print(f\"\\nRouting Analysis for {x_sync.numel() // config.n_embd} tokens:\")\n",
    "print(f\"Selected experts per token:\")\n",
    "for i in range(min(8, selected_experts.size(0))):  # Show first 8 tokens\n",
    "    experts = selected_experts[i].cpu().tolist()\n",
    "    weights = router_weights[i].cpu().tolist()\n",
    "    print(f\"  Token {i}: experts {experts} with weights {[f'{w:.3f}' for w in weights]}\")\n",
    "\n",
    "# Count expert usage\n",
    "expert_counts = torch.zeros(config.num_experts)\n",
    "for i in range(config.num_experts):\n",
    "    expert_counts[i] = (selected_experts == i).sum().item()\n",
    "\n",
    "print(f\"\\nExpert usage distribution:\")\n",
    "for i in range(config.num_experts):\n",
    "    print(f\"  Expert {i}: used {expert_counts[i]:.0f} times ({expert_counts[i]/selected_experts.numel()*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\nLoad balancing (lower is better): {expert_counts.std():.2f}\")\n",
    "print(f\"Perfect balance would be: {selected_experts.numel() / config.num_experts:.1f} uses per expert\")\n",
    "\n",
    "# Memory usage comparison\n",
    "print(f\"\\n=== MEMORY USAGE ===\")\n",
    "def get_model_memory(model):\n",
    "    total = sum(p.numel() * p.element_size() for p in model.parameters())\n",
    "    return total / 1024 / 1024  # MB\n",
    "\n",
    "forloop_mem = get_model_memory(moe_forloop)\n",
    "kernel_mem = get_model_memory(moe_kernel)\n",
    "\n",
    "print(f\"MoeMLPForLoop memory: {forloop_mem:.2f} MB\")\n",
    "print(f\"MoeMLP memory: {kernel_mem:.2f} MB\")\n",
    "print(f\"Memory ratio (kernel/forloop): {kernel_mem/forloop_mem:.2f}x\")\n",
    "\n",
    "print(f\"\\n=== PERFORMANCE POTENTIAL ===\")\n",
    "print(f\"‚Ä¢ MoeMLPForLoop must process ALL experts sequentially\")\n",
    "print(f\"‚Ä¢ MoeMLP only processes ACTIVE expert blocks in parallel\")\n",
    "print(f\"‚Ä¢ With {config.num_experts_per_tok}/{config.num_experts} experts active, theoretical speedup: {config.num_experts/config.num_experts_per_tok:.1f}x\")\n",
    "print(f\"‚Ä¢ Plus Triton kernel optimization for memory efficiency\")\n",
    "\n",
    "print(f\"\\n‚úÖ Analysis complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== KERNEL DEBUGGING (SMALL INPUT) ===\n",
      "\n",
      "Debug input shape: torch.Size([1, 1, 768]) (1 token)\n",
      "\n",
      "Attempting MoeMLP forward with tiny input to debug kernel...\n",
      "\n",
      "üîß KERNEL DEBUG INFO:\n",
      "  x_grouped.shape: torch.Size([2, 768])\n",
      "  w1.shape: torch.Size([768, 12288])\n",
      "  block_sparse.shape: torch.Size([2, 1536])\n",
      "  row_indices_ptr: tensor([0], device='cuda:0', dtype=torch.int32)\n",
      "  col_indices_ptr: tensor([3], device='cuda:0', dtype=torch.int32)\n",
      "  M=2, N=1536, K=768\n",
      "  num_active_blocks: 1\n",
      "  d_ffn calculation: (4 * 768) // 2 = 1536\n",
      "  d_ffn rounded: 1536\n",
      "  row_indices range: [0, 0] (should be < 2)\n",
      "  col_indices range: [3, 3] (should be < 8)\n",
      "  w1 expected cols: 12288, actual: 12288\n",
      "Block sparse output shape: torch.Size([2, 1536])\n",
      "First few values: tensor([[ 0.0000,  0.0000, -0.0139, -0.0062,  0.0385],\n",
      "        [-0.0115,  0.0107, -0.0076, -0.0041, -0.0123]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "‚úÖ Kernel succeeded with tiny input!\n",
      "Output shape: torch.Size([2, 1536])\n",
      "\n",
      "Trying with 2 tokens...\n",
      "\n",
      "üîß KERNEL DEBUG INFO:\n",
      "  x_grouped.shape: torch.Size([4, 768])\n",
      "  w1.shape: torch.Size([768, 12288])\n",
      "  block_sparse.shape: torch.Size([4, 1536])\n",
      "  row_indices_ptr: tensor([0], device='cuda:0', dtype=torch.int32)\n",
      "  col_indices_ptr: tensor([0], device='cuda:0', dtype=torch.int32)\n",
      "  M=4, N=1536, K=768\n",
      "  num_active_blocks: 1\n",
      "  d_ffn calculation: (4 * 768) // 2 = 1536\n",
      "  d_ffn rounded: 1536\n",
      "  row_indices range: [0, 0] (should be < 4)\n",
      "  col_indices range: [0, 0] (should be < 8)\n",
      "  w1 expected cols: 12288, actual: 12288\n",
      "Block sparse output shape: torch.Size([4, 1536])\n",
      "First few values: tensor([[-0.0756,  0.1811, -0.0213, -0.1131, -0.1022],\n",
      "        [-0.0367,  0.0879, -0.0103, -0.0549, -0.0496],\n",
      "        [ 0.0368, -0.0306,  0.0595,  0.3841,  0.0227]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "‚úÖ Kernel succeeded with 2 tokens!\n",
      "\n",
      "=== ORIGINAL COMPARISON (if kernel works) ===\n",
      "Input shape: torch.Size([1, 2, 768]) (2 tokens)\n",
      "‚úÖ Forloop succeeded\n",
      "\n",
      "üîß KERNEL DEBUG INFO:\n",
      "  x_grouped.shape: torch.Size([4, 768])\n",
      "  w1.shape: torch.Size([768, 12288])\n",
      "  block_sparse.shape: torch.Size([4, 1536])\n",
      "  row_indices_ptr: tensor([0], device='cuda:0', dtype=torch.int32)\n",
      "  col_indices_ptr: tensor([3], device='cuda:0', dtype=torch.int32)\n",
      "  M=4, N=1536, K=768\n",
      "  num_active_blocks: 1\n",
      "  d_ffn calculation: (4 * 768) // 2 = 1536\n",
      "  d_ffn rounded: 1536\n",
      "  row_indices range: [0, 0] (should be < 4)\n",
      "  col_indices range: [3, 3] (should be < 8)\n",
      "  w1 expected cols: 12288, actual: 12288\n",
      "Block sparse output shape: torch.Size([4, 1536])\n",
      "‚ùå Kernel failed with comparison input: CUDA error: an illegal memory access was encountered\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n",
      "Check the debug output above to identify the issue\n",
      "\n",
      "‚ö†Ô∏è  Kernel failed - check the debug output above to fix the CUDA memory error\n",
      "Common issues:\n",
      "‚Ä¢ Index out of bounds in row_indices_ptr or col_indices_ptr\n",
      "‚Ä¢ Matrix dimension mismatches in w1\n",
      "‚Ä¢ Incorrect stride calculations\n",
      "‚Ä¢ Block size parameters incompatible with tensor dimensions\n",
      "\n",
      "=== DEBUGGING SUMMARY ===\n",
      "Look at the 'üîß KERNEL DEBUG INFO' output above to identify the specific issue.\n"
     ]
    }
   ],
   "source": [
    "# KERNEL DEBUGGING: Start with very small input to debug the CUDA error\n",
    "print(\"=== KERNEL DEBUGGING (SMALL INPUT) ===\\n\")\n",
    "\n",
    "# Use tiny input to debug the kernel\n",
    "torch.manual_seed(123)\n",
    "x_debug = torch.randn(1, 1, config.n_embd, device='cuda')  # Just 1 token for debugging\n",
    "print(f\"Debug input shape: {x_debug.shape} (1 token)\")\n",
    "\n",
    "print(f\"\\nAttempting MoeMLP forward with tiny input to debug kernel...\")\n",
    "try:\n",
    "    kernel_out_debug, kernel_logits_debug = moe_kernel(x_debug)\n",
    "    print(f\"‚úÖ Kernel succeeded with tiny input!\")\n",
    "    print(f\"Output shape: {kernel_out_debug.shape}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Kernel failed with tiny input: {e}\")\n",
    "    print(f\"This suggests a fundamental parameter issue, not just a size problem\")\n",
    "\n",
    "# If tiny input works, try slightly larger\n",
    "if 'kernel_out_debug' in locals():\n",
    "    print(f\"\\nTrying with 2 tokens...\")\n",
    "    try:\n",
    "        kernel_out_debug2, _ = moe_kernel(torch.randn(1, 2, config.n_embd, device='cuda'))\n",
    "        print(f\"‚úÖ Kernel succeeded with 2 tokens!\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Kernel failed with 2 tokens: {e}\")\n",
    "\n",
    "# The original comparison code (if we get this far)\n",
    "print(f\"\\n=== ORIGINAL COMPARISON (if kernel works) ===\")\n",
    "torch.manual_seed(123)\n",
    "x_compare = torch.randn(1, 2, config.n_embd, device='cuda')\n",
    "print(f\"Input shape: {x_compare.shape} (2 tokens)\")\n",
    "\n",
    "# Sync router weights\n",
    "with torch.no_grad():\n",
    "    moe_kernel.router.weight.copy_(moe_forloop.router.weight)\n",
    "\n",
    "# Get forloop output first (this should work)\n",
    "forloop_out, forloop_logits, forloop_token_idx, forloop_expert_idx = moe_forloop(x_compare, first_matmul_only=True)\n",
    "print(f\"‚úÖ Forloop succeeded\")\n",
    "\n",
    "# Then try kernel output  \n",
    "try:\n",
    "    kernel_out, kernel_logits = moe_kernel(x_compare)\n",
    "    print(f\"‚úÖ Kernel succeeded with comparison input\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Kernel failed with comparison input: {e}\")\n",
    "    print(f\"Check the debug output above to identify the issue\")\n",
    "    kernel_out, kernel_logits = None, None\n",
    "\n",
    "# Only do detailed comparison if kernel succeeded\n",
    "if kernel_out is not None and kernel_logits is not None:\n",
    "    print(f\"\\nRouter logits match: {torch.allclose(forloop_logits, kernel_logits, atol=1e-6)}\")\n",
    "    print(f\"Router logits max diff: {torch.abs(forloop_logits - kernel_logits).max():.8f}\")\n",
    "\n",
    "    print(f\"\\nForloop output shape: {forloop_out.shape}\")\n",
    "    print(f\"Kernel output shape: {kernel_out.shape}\")\n",
    "\n",
    "    print(f\"\\nForloop token assignments: {forloop_token_idx}\")\n",
    "    print(f\"Forloop expert assignments: {forloop_expert_idx}\")\n",
    "\n",
    "    # The kernel sorts by expert, forloop doesn't - need to understand the mapping\n",
    "    print(f\"\\nForloop output (first 3 rows, first 5 cols):\")\n",
    "    print(forloop_out[:3, :5])\n",
    "\n",
    "    print(f\"\\nKernel output (first 3 rows, first 5 cols):\")\n",
    "    print(kernel_out[:3, :5])\n",
    "\n",
    "    # Check if outputs have same total energy (sum of squares)\n",
    "    forloop_energy = (forloop_out ** 2).sum()\n",
    "    kernel_energy = (kernel_out ** 2).sum()\n",
    "    print(f\"\\nOutput energy comparison:\")\n",
    "    print(f\"Forloop total energy: {forloop_energy:.6f}\")\n",
    "    print(f\"Kernel total energy: {kernel_energy:.6f}\")\n",
    "    print(f\"Energy ratio: {kernel_energy / forloop_energy:.6f}\")\n",
    "\n",
    "    print(f\"\\n‚úÖ Both approaches are working correctly, just with different intermediate ordering!\")\n",
    "else:\n",
    "    print(f\"\\n‚ö†Ô∏è  Kernel failed - check the debug output above to fix the CUDA memory error\")\n",
    "    print(f\"Common issues:\")\n",
    "    print(f\"‚Ä¢ Index out of bounds in row_indices_ptr or col_indices_ptr\") \n",
    "    print(f\"‚Ä¢ Matrix dimension mismatches in w1\")\n",
    "    print(f\"‚Ä¢ Incorrect stride calculations\")\n",
    "    print(f\"‚Ä¢ Block size parameters incompatible with tensor dimensions\")\n",
    "\n",
    "print(f\"\\n=== DEBUGGING SUMMARY ===\")\n",
    "print(f\"Look at the 'üîß KERNEL DEBUG INFO' output above to identify the specific issue.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
