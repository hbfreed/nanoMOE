{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = \"cuda:1\" if torch.cuda.is_available() else \"cpu\"\n",
    "n_embd = 8\n",
    "selected_experts_sorted = torch.tensor(\n",
    "    [0,0,0,0,0, 1,1, 2], dtype=torch.long, device=device\n",
    ")\n",
    "\n",
    "def _pad_to_blocks(x_sorted: torch.Tensor,\n",
    "                   selected_experts_sorted: torch.Tensor,\n",
    "                   block_size: int = 4):\n",
    "    \"\"\"\n",
    "    Standalone version. Assumes selected_experts_sorted is grouped (ascending by expert id).\n",
    "    Returns: x_padded, tokens_per_expert_padded, unpad_indices\n",
    "    \"\"\"\n",
    "    device = x_sorted.device\n",
    "    n_embd = x_sorted.size(-1)\n",
    "    num_experts = int(selected_experts_sorted.max().item()) + 1 if selected_experts_sorted.numel() else 0\n",
    "    tokens_per_expert = torch.zeros(num_experts, dtype=torch.long, device=device)\n",
    "    ones = torch.ones_like(selected_experts_sorted, dtype=torch.long)\n",
    "\n",
    "    tokens_per_expert.scatter_add_(0, selected_experts_sorted, ones)\n",
    "    tokens_per_expert_padded = ((tokens_per_expert + block_size - 1) // block_size) * block_size\n",
    "\n",
    "    cumsum_original = F.pad(tokens_per_expert.cumsum(0), (1, 0))\n",
    "    cumsum_padded   = F.pad(tokens_per_expert_padded.cumsum(0), (1, 0))\n",
    "    total_padded_tokens = cumsum_padded[-1]\n",
    "\n",
    "    x_padded = torch.zeros((total_padded_tokens, n_embd), dtype=x_sorted.dtype, device=device)\n",
    "\n",
    "    original_positions = torch.arange(len(x_sorted),device=x_sorted.device)\n",
    "    padded_positions = (original_positions - cumsum_original[selected_experts_sorted]) + cumsum_padded[selected_experts_sorted]\n",
    "    x_padded[padded_positions] = x_sorted\n",
    "\n",
    "    unpad_indices = padded_positions\n",
    "    return x_padded, tokens_per_expert_padded, unpad_indices\n",
    "\n",
    "x_sorted = torch.randn(selected_experts_sorted.numel(), n_embd, device=device)\n",
    "\n",
    "# example call\n",
    "x_padded, tokens_per_expert_padded, unpad_indices = _pad_to_blocks(\n",
    "    x_sorted, selected_experts_sorted\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "row_indices: tensor([0, 0, 0, 1, 1, 1, 2, 2, 2, 3, 3, 3, 4, 4, 4], device='cuda:1',\n",
      "       dtype=torch.int32)\n",
      "weight_col_indices: tensor([0, 1, 2, 0, 1, 2, 3, 4, 5, 3, 4, 5, 6, 7, 8], device='cuda:1',\n",
      "       dtype=torch.int32)\n",
      "output_col_indices: tensor([0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2], device='cuda:1',\n",
      "       dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = \"cuda:1\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Setup parameters\n",
    "num_experts = 3\n",
    "block_size = 4\n",
    "d_ffn = 12  # FFN dimension (will need 3 blocks of size 4)\n",
    "\n",
    "# From your previous function, we'd have tokens_per_expert_padded\n",
    "# Let's say expert 0 got 8 tokens, expert 1 got 8, expert 2 got 4\n",
    "tokens_per_expert_padded = torch.tensor([8, 8, 4], dtype=torch.long, device=device)\n",
    "\n",
    "def _create_sparse_indices(tokens_per_expert_padded, num_experts, block_size, d_ffn):\n",
    "    \"\"\"Create row and column indices for sparse blocks.\"\"\"\n",
    "    device = tokens_per_expert_padded.device\n",
    "    num_token_blocks_per_expert = tokens_per_expert_padded // block_size\n",
    "    num_ffn_blocks = (d_ffn + block_size - 1) // block_size\n",
    "    \n",
    "    blocks_per_expert = num_token_blocks_per_expert * num_ffn_blocks\n",
    "    \n",
    "    expert_ids = torch.repeat_interleave(\n",
    "        torch.arange(num_experts, device=device),\n",
    "        blocks_per_expert\n",
    "    )\n",
    "    within_expert_block_idx = torch.arange(len(expert_ids),device=device) - F.pad(blocks_per_expert.cumsum(0)[:-1], (1,0))[expert_ids]\n",
    "    \n",
    "\n",
    "    token_block_offset = F.pad(num_token_blocks_per_expert.cumsum(0)[:-1], (1, 0))\n",
    "    row_indices = token_block_offset[expert_ids] + (within_expert_block_idx // num_ffn_blocks)\n",
    "    weight_col_indices = expert_ids * num_ffn_blocks + (within_expert_block_idx % num_ffn_blocks)\n",
    "    output_col_indices = within_expert_block_idx % num_ffn_blocks\n",
    "    \n",
    "    return row_indices.int(), weight_col_indices.int(), output_col_indices.int()\n",
    "\n",
    "# Test it\n",
    "row_idx, weight_col_idx, output_col_idx = _create_sparse_indices(\n",
    "    tokens_per_expert_padded, num_experts, block_size, d_ffn\n",
    ")\n",
    "\n",
    "print(f\"\\nrow_indices: {row_idx}\")\n",
    "print(f\"weight_col_indices: {weight_col_idx}\")\n",
    "print(f\"output_col_indices: {output_col_idx}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"PyTorch implementation of what we're trying to do in dsd\"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "num_tokens = 512\n",
    "hidden_size = 768\n",
    "d_ffn = 1536\n",
    "num_experts = 8\n",
    "num_active_experts = 2\n",
    "block_size = 16\n",
    "num_tokens_per_expert = num_tokens//num_experts #not realistic! but this is a toy scenario\n",
    "\n",
    "x = torch.randn(num_tokens, d_ffn, device='cuda', dtype=torch.bfloat16) #this is x *after* w1\n",
    "\n",
    "w2 = torch.randn(d_ffn * num_experts, hidden_size, device='cuda', dtype=torch.bfloat16) # w2 with num_experts experts\n",
    "\n",
    "'''\n",
    "Now, let's say that all the tokens get distributed to each expert evenly.\n",
    "Then we'll get batch_size/num_experts = block_size tokens to each expert. Let's just do it in a for loop, but we know we'd be doing it in parallel in triton\n",
    "'''\n",
    "\n",
    "out_tensor = torch.zeros(num_tokens, hidden_size, device='cuda',dtype=torch.bfloat16) #accumulating to this tensor, so use zeros\n",
    "for expert_idx in range(num_experts):\n",
    "    x_bottom_index = expert_idx * num_tokens_per_expert\n",
    "    x_top_index = (expert_idx + 1) * num_tokens_per_expert\n",
    "    w2_bottom_index = expert_idx * d_ffn\n",
    "    w2_top_index = (expert_idx + 1) * d_ffn\n",
    "    x_expert = x[x_bottom_index:x_top_index]\n",
    "    w2_expert = w2[w2_bottom_index:w2_top_index]\n",
    "    output_block = torch.zeros(num_tokens_per_expert, hidden_size, device='cuda', dtype=torch.bfloat16)\n",
    "    for k in range(0, d_ffn, block_size): # d_ffn is NOT divisible by block_size, so we'll have to mask! just like triton.\n",
    "        k_end = min(k + block_size, d_ffn)\n",
    "\n",
    "        x_tile = x_expert[:, k:k_end]\n",
    "        w2_tile = w2_expert[k:k_end,:]\n",
    "\n",
    "        output_block += x_tile @ w2_tile\n",
    "\n",
    "    out_tensor[x_bottom_index:x_top_index] = output_block\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xp: torch.Size([128, 768])\n",
      "torch.Size([16, 768])\n",
      "bs: torch.Size([128, 1536])\n",
      "row_indices: torch.Size([768])\n",
      "weight_col_indices: torch.Size([768])\n",
      "output_col_indices: torch.Size([768])\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from model import MoeMLP, GPTConfig\n",
    "\n",
    "torch.manual_seed(0)\n",
    "\n",
    "config = GPTConfig()\n",
    "config.n_embd = 768\n",
    "config.num_experts = 8 \n",
    "config.num_experts_per_tok = 2\n",
    "config.n_ctx = 8\n",
    "config.block_k,config.block_size = 16,16\n",
    "\n",
    "model = MoeMLP(config).cuda().bfloat16()\n",
    "x = torch.randn(1, 8, 768, device='cuda', dtype=torch.bfloat16)\n",
    "\n",
    "out = model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Script to compare the outputs of MoeMLP (Triton kernel) and MoeMlpForLoop (Python loop)\n",
    "to identify discrepancies in the Triton kernel implementation.\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from dataclasses import dataclass\n",
    "from typing import Tuple, Dict\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Add parent directory to path to import the model\n",
    "sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))\n",
    "\n",
    "from model import MoeMLP, MoeMLPForLoop\n",
    "\n",
    "@dataclass\n",
    "class TestConfig:\n",
    "    \"\"\"Configuration for testing the MoE models.\"\"\"\n",
    "    n_embd: int = 384\n",
    "    n_ctx: int = 256  # sequence length\n",
    "    num_experts: int = 1\n",
    "    num_experts_per_tok: int = 1\n",
    "    norm_topk_prob: bool = True\n",
    "    bias: bool = False\n",
    "    # Triton block config (match MoeMLP defaults)\n",
    "    block_size: int = 16 #will be \"slow\" but don't need to slog through all that padding\n",
    "    block_k: int = 16\n",
    "    \n",
    "    # For compatibility with MLP initialization\n",
    "    dropout: float = 0.0\n",
    "\n",
    "def create_test_input(batch_size: int, seq_len: int, hidden_dim: int, device='cuda', seed=42):\n",
    "    \"\"\"Create reproducible test input.\"\"\"\n",
    "    torch.manual_seed(seed)\n",
    "    return torch.randn(batch_size, seq_len, hidden_dim, device=device, dtype=torch.float32)\n",
    "\n",
    "def compare_outputs(out1: torch.Tensor, out2: torch.Tensor, name: str, rtol=1e-4, atol=1e-5):\n",
    "    \"\"\"Compare two tensors and report differences.\"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Comparing: {name}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Basic shape check\n",
    "    if out1.shape != out2.shape:\n",
    "        print(f\"‚ùå Shape mismatch: {out1.shape} vs {out2.shape}\")\n",
    "        return False\n",
    "    \n",
    "    print(f\"‚úì Shape match: {out1.shape}\")\n",
    "    \n",
    "    # Compute differences\n",
    "    diff = (out1 - out2).abs()\n",
    "    rel_diff = diff / (out2.abs() + 1e-8)\n",
    "    \n",
    "    # Statistics\n",
    "    print(f\"\\nAbsolute difference stats:\")\n",
    "    print(f\"  Max: {diff.max().item():.2e}\")\n",
    "    print(f\"  Mean: {diff.mean().item():.2e}\")\n",
    "    print(f\"  Std: {diff.std().item():.2e}\")\n",
    "    \n",
    "    print(f\"\\nRelative difference stats:\")\n",
    "    print(f\"  Max: {rel_diff.max().item():.2e}\")\n",
    "    print(f\"  Mean: {rel_diff.mean().item():.2e}\")\n",
    "    print(f\"  Std: {rel_diff.std().item():.2e}\")\n",
    "    \n",
    "    # Check if outputs are close\n",
    "    is_close = torch.allclose(out1, out2, rtol=rtol, atol=atol)\n",
    "    \n",
    "    if is_close:\n",
    "        print(f\"‚úì Outputs match within tolerance (rtol={rtol}, atol={atol})\")\n",
    "    else:\n",
    "        print(f\"‚ùå Outputs differ beyond tolerance (rtol={rtol}, atol={atol})\")\n",
    "        \n",
    "        # Find worst mismatches\n",
    "        n_worst = min(5, diff.numel())\n",
    "        diff_flat = diff.flatten()\n",
    "        worst_indices = torch.topk(diff_flat, n_worst).indices\n",
    "        \n",
    "        print(f\"\\nTop {n_worst} worst absolute differences:\")\n",
    "        for i, idx in enumerate(worst_indices):\n",
    "            unraveled = np.unravel_index(idx.cpu().item(), out1.shape)\n",
    "            val1 = out1[unraveled].item()\n",
    "            val2 = out2[unraveled].item()\n",
    "            print(f\"  {i+1}. Index {unraveled}: {val1:.6f} vs {val2:.6f} (diff: {abs(val1-val2):.2e})\")\n",
    "    \n",
    "    return is_close\n",
    "\n",
    "def test_forward_pass(config: TestConfig, batch_size=2, seq_len=128, device='cuda'):\n",
    "    \"\"\"Test forward pass of both models.\"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"TESTING FORWARD PASS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Create models\n",
    "    print(\"\\nInitializing models...\")\n",
    "    moe_triton = MoeMLP(config).to(device)\n",
    "    moe_loop = MoeMLPForLoop(config).to(device)\n",
    "    \n",
    "    # Copy weights from triton model to loop model to ensure same initialization\n",
    "    print(\"Synchronizing weights...\")\n",
    "    with torch.no_grad():\n",
    "        # Copy router weights\n",
    "        moe_loop.router.weight.copy_(moe_triton.router.weight)\n",
    "        \n",
    "        # Copy expert weights from Triton's combined weights to loop's individual experts\n",
    "        d_ffn_per_expert = moe_triton.d_ffn\n",
    "        for i in range(config.num_experts):\n",
    "            # Extract this expert's weights from the combined tensors\n",
    "            w1_slice = moe_triton.w1[:, i*d_ffn_per_expert:(i+1)*d_ffn_per_expert]\n",
    "            w2_slice = moe_triton.w2[i*d_ffn_per_expert:(i+1)*d_ffn_per_expert, :]\n",
    "            \n",
    "            # The loop model uses standard MLP with c_fc and c_proj\n",
    "            # MLP structure: c_fc projects n_embd -> 4*n_embd, c_proj projects 4*n_embd -> n_embd\n",
    "            # But we need to handle the dimension mismatch since MoeMLP uses d_ffn\n",
    "            \n",
    "            # Get the actual dimensions from the loop model's experts\n",
    "            expert_mlp = moe_loop.experts[i]\n",
    "            fc_out_dim = expert_mlp.c_fc.weight.shape[0]\n",
    "            \n",
    "            # Only copy the portions that fit\n",
    "            copy_dim = min(w1_slice.shape[1], fc_out_dim)\n",
    "            expert_mlp.c_fc.weight[:copy_dim, :].copy_(w1_slice.T[:copy_dim, :])\n",
    "            expert_mlp.c_proj.weight[:, :copy_dim].copy_(w2_slice[:copy_dim, :].T)\n",
    "    \n",
    "    # Create test input\n",
    "    print(f\"\\nCreating test input: batch={batch_size}, seq={seq_len}, hidden={config.n_embd}\")\n",
    "    x = create_test_input(batch_size, seq_len, config.n_embd, device=device)\n",
    "    \n",
    "    # Forward pass\n",
    "    print(\"\\nRunning forward passes...\")\n",
    "    with torch.no_grad():\n",
    "        # Triton model\n",
    "        out_triton, aux_triton, f_i_triton = moe_triton(x)\n",
    "        \n",
    "        # Loop model  \n",
    "        out_loop, aux_loop, f_i_loop = moe_loop(x)\n",
    "    \n",
    "    # Compare main outputs\n",
    "    outputs_match = compare_outputs(out_triton, out_loop, \"Main Output\")\n",
    "    \n",
    "    # Compare auxiliary losses\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"Auxiliary Losses Comparison\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    print(f\"\\nRouter Z-Loss:\")\n",
    "    print(f\"  Triton: {aux_triton['router_z_loss'].item():.6f}\")\n",
    "    print(f\"  Loop:   {aux_loop['router_z_loss'].item():.6f}\")\n",
    "    print(f\"  Diff:   {abs(aux_triton['router_z_loss'].item() - aux_loop['router_z_loss'].item()):.2e}\")\n",
    "    \n",
    "    print(f\"\\nLoad Balance Loss:\")\n",
    "    print(f\"  Triton: {aux_triton['load_balance_loss'].item():.6f}\")\n",
    "    print(f\"  Loop:   {aux_loop['load_balance_loss'].item():.6f}\")\n",
    "    print(f\"  Diff:   {abs(aux_triton['load_balance_loss'].item() - aux_loop['load_balance_loss'].item()):.2e}\")\n",
    "    \n",
    "    # Compare expert utilization\n",
    "    f_i_match = compare_outputs(f_i_triton, f_i_loop, \"Expert Utilization (f_i)\")\n",
    "    \n",
    "    return outputs_match and f_i_match\n",
    "\n",
    "def test_gradient_flow(config: TestConfig, batch_size=2, seq_len=64, device='cuda'):\n",
    "    \"\"\"Test backward pass and gradient computation.\"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"TESTING GRADIENT FLOW\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Create models\n",
    "    print(\"\\nInitializing models...\")\n",
    "    moe_triton = MoeMLP(config).to(device)\n",
    "    moe_loop = MoeMLPForLoop(config).to(device)\n",
    "    \n",
    "    # Synchronize weights as before\n",
    "    print(\"Synchronizing weights...\")\n",
    "    with torch.no_grad():\n",
    "        moe_loop.router.weight.copy_(moe_triton.router.weight)\n",
    "        \n",
    "        d_ffn_per_expert = moe_triton.d_ffn\n",
    "        for i in range(config.num_experts):\n",
    "            w1_slice = moe_triton.w1[:, i*d_ffn_per_expert:(i+1)*d_ffn_per_expert]\n",
    "            w2_slice = moe_triton.w2[i*d_ffn_per_expert:(i+1)*d_ffn_per_expert, :]\n",
    "            \n",
    "            expert_mlp = moe_loop.experts[i]\n",
    "            fc_out_dim = expert_mlp.c_fc.weight.shape[0]\n",
    "            \n",
    "            copy_dim = min(w1_slice.shape[1], fc_out_dim)\n",
    "            expert_mlp.c_fc.weight[:copy_dim, :].copy_(w1_slice.T[:copy_dim, :])\n",
    "            expert_mlp.c_proj.weight[:, :copy_dim].copy_(w2_slice[:copy_dim, :].T)\n",
    "    \n",
    "    # Create test input\n",
    "    print(f\"\\nCreating test input: batch={batch_size}, seq={seq_len}, hidden={config.n_embd}\")\n",
    "    x = create_test_input(batch_size, seq_len, config.n_embd, device=device)\n",
    "    x_triton = x.clone().requires_grad_(True)\n",
    "    x_loop = x.clone().requires_grad_(True)\n",
    "    \n",
    "    # Forward pass\n",
    "    print(\"\\nRunning forward passes with gradient tracking...\")\n",
    "    out_triton, aux_triton, _ = moe_triton(x_triton)\n",
    "    out_loop, aux_loop, _ = moe_loop(x_loop)\n",
    "    \n",
    "    # Create dummy loss\n",
    "    loss_triton = out_triton.mean() + 0.01 * aux_triton['load_balance_loss']\n",
    "    loss_loop = out_loop.mean() + 0.01 * aux_loop['load_balance_loss']\n",
    "    \n",
    "    # Backward pass\n",
    "    print(\"Running backward passes...\")\n",
    "    loss_triton.backward()\n",
    "    loss_loop.backward()\n",
    "    \n",
    "    # Compare input gradients\n",
    "    grad_match = compare_outputs(x_triton.grad, x_loop.grad, \"Input Gradients\")\n",
    "    \n",
    "    # Compare router gradients\n",
    "    router_grad_match = compare_outputs(\n",
    "        moe_triton.router.weight.grad,\n",
    "        moe_loop.router.weight.grad,\n",
    "        \"Router Weight Gradients\"\n",
    "    )\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"Weight Gradient Statistics\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Check W1 gradients\n",
    "    if moe_triton.w1.grad is not None:\n",
    "        print(f\"\\nW1 gradients (Triton):\")\n",
    "        print(f\"  Shape: {moe_triton.w1.grad.shape}\")\n",
    "        print(f\"  Mean: {moe_triton.w1.grad.mean().item():.2e}\")\n",
    "        print(f\"  Std:  {moe_triton.w1.grad.std().item():.2e}\")\n",
    "        print(f\"  Max:  {moe_triton.w1.grad.abs().max().item():.2e}\")\n",
    "    \n",
    "    # Check W2 gradients\n",
    "    if moe_triton.w2.grad is not None:\n",
    "        print(f\"\\nW2 gradients (Triton):\")\n",
    "        print(f\"  Shape: {moe_triton.w2.grad.shape}\")\n",
    "        print(f\"  Mean: {moe_triton.w2.grad.mean().item():.2e}\")\n",
    "        print(f\"  Std:  {moe_triton.w2.grad.std().item():.2e}\")\n",
    "        print(f\"  Max:  {moe_triton.w2.grad.abs().max().item():.2e}\")\n",
    "    \n",
    "    return grad_match and router_grad_match\n",
    "\n",
    "def test_edge_cases(config: TestConfig, device='cuda'):\n",
    "    \"\"\"Test edge cases and special scenarios.\"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"TESTING EDGE CASES\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    test_cases = [\n",
    "        (\"Single token\", 1, 1),\n",
    "        (\"Single batch\", 1, 64),\n",
    "        (\"Large batch\", 8, 32),\n",
    "        (\"Non-power-of-2 seq\", 2, 97),\n",
    "    ]\n",
    "    \n",
    "    all_pass = True\n",
    "    \n",
    "    for name, batch_size, seq_len in test_cases:\n",
    "        print(f\"\\n\\nTest: {name} (batch={batch_size}, seq={seq_len})\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        try:\n",
    "            moe_triton = MoeMLP(config).to(device)\n",
    "            moe_loop = MoeMLPForLoop(config).to(device)\n",
    "            \n",
    "            # Sync weights\n",
    "            with torch.no_grad():\n",
    "                moe_loop.router.weight.copy_(moe_triton.router.weight)\n",
    "                d_ffn_per_expert = moe_triton.d_ffn\n",
    "                for i in range(config.num_experts):\n",
    "                    w1_slice = moe_triton.w1[:, i*d_ffn_per_expert:(i+1)*d_ffn_per_expert]\n",
    "                    w2_slice = moe_triton.w2[i*d_ffn_per_expert:(i+1)*d_ffn_per_expert, :]\n",
    "                    expert_mlp = moe_loop.experts[i]\n",
    "                    fc_out_dim = expert_mlp.c_fc.weight.shape[0]\n",
    "                    copy_dim = min(w1_slice.shape[1], fc_out_dim)\n",
    "                    expert_mlp.c_fc.weight[:copy_dim, :].copy_(w1_slice.T[:copy_dim, :])\n",
    "                    expert_mlp.c_proj.weight[:, :copy_dim].copy_(w2_slice[:copy_dim, :].T)\n",
    "            \n",
    "            x = create_test_input(batch_size, seq_len, config.n_embd, device=device)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                out_triton, _, _ = moe_triton(x)\n",
    "                out_loop, _, _ = moe_loop(x)\n",
    "            \n",
    "            if torch.allclose(out_triton, out_loop, rtol=1e-3, atol=1e-4):\n",
    "                print(f\"‚úì PASS\")\n",
    "            else:\n",
    "                print(f\"‚ùå FAIL - Max diff: {(out_triton - out_loop).abs().max().item():.2e}\")\n",
    "                all_pass = False\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå ERROR: {str(e)}\")\n",
    "            all_pass = False\n",
    "    \n",
    "    return all_pass\n",
    "\n",
    "def main():\n",
    "    # Check CUDA availability\n",
    "    if not torch.cuda.is_available():\n",
    "        print(\"CUDA not available. Running on CPU (will be slow).\")\n",
    "        device = 'cpu'\n",
    "    else:\n",
    "        device = 'cuda'\n",
    "        print(f\"Using device: {torch.cuda.get_device_name()}\")\n",
    "    \n",
    "    # Create test configuration\n",
    "    config = TestConfig()\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"MOE IMPLEMENTATION COMPARISON TEST\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"\\nConfiguration:\")\n",
    "    print(f\"  Hidden dimension: {config.n_embd}\")\n",
    "    print(f\"  Sequence length: {config.n_ctx}\")\n",
    "    print(f\"  Number of experts: {config.num_experts}\")\n",
    "    print(f\"  Experts per token: {config.num_experts_per_tok}\")\n",
    "    print(f\"  Normalize top-k: {config.norm_topk_prob}\")\n",
    "    \n",
    "    # Run tests\n",
    "    results = {}\n",
    "    \n",
    "    # Test forward pass\n",
    "    results['forward'] = test_forward_pass(config, device=device)\n",
    "    \n",
    "    # Test gradient flow\n",
    "    # results['gradient'] = test_gradient_flow(config, device=device)\n",
    "    \n",
    "    # Test edge cases\n",
    "    # results['edge_cases'] = test_edge_cases(config, device=device)\n",
    "    \n",
    "    # Summary\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"SUMMARY\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    for test_name, passed in results.items():\n",
    "        status = \"‚úì PASSED\" if passed else \"‚ùå FAILED\"\n",
    "        print(f\"  {test_name.capitalize():15} {status}\")\n",
    "    \n",
    "    all_passed = all(results.values())\n",
    "    \n",
    "    if all_passed:\n",
    "        print(\"\\nüéâ All tests passed! The implementations match.\")\n",
    "    else:\n",
    "        print(\"\\n‚ö†Ô∏è  Some tests failed. There are discrepancies between implementations.\")\n",
    "    \n",
    "    return 0 if all_passed else 1\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    exit(main())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
