{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from dataclasses import dataclass\n",
    "from einops import rearrange\n",
    "from moe import sdd_kernel\n",
    "from model import MoeMLP\n",
    "@dataclass\n",
    "class ToyMoEConfig:\n",
    "    n_embd: int = 128\n",
    "    num_experts: int = 4\n",
    "    num_experts_per_tok: int = 2\n",
    "    norm_topk_prob: bool = True\n",
    "    bias: bool = True\n",
    "    dropout: float = 0.0\n",
    "config = ToyMoEConfig()\n",
    "# print(f\"Config: {config}\")\n",
    "\n",
    "# Create toy input\n",
    "batch_size = 4\n",
    "seq_len = 8\n",
    "x = torch.randn(batch_size, seq_len, config.n_embd,device='cuda')\n",
    "# print(f\"\\nInput shape: {x.shape}\")\n",
    "\n",
    "# Create MoeMLP instance\n",
    "moe_mlp = MoeMLP(config).cuda()\n",
    "# print(f\"\\nMoeMLP d_ffn: {moe_mlp.d_ffn}\")\n",
    "# print(f\"w1 shape: {moe_mlp.w1.shape}\")\n",
    "# print(f\"w2 shape: {moe_mlp.w2.shape}\")\n",
    "\n",
    "# Test forward pass\n",
    "print(f\"\\n=== Forward Pass ===\")\n",
    "block_sparse, router_logits, debug_info = moe_mlp(x)\n",
    "print(f\"\\nForward pass completed!\")\n",
    "print(f\"Router logits shape: {router_logits.shape}\")\n",
    "print(f\"Block sparse shape: {block_sparse.shape}\")\n",
    "\n",
    "# Extract debug variables\n",
    "col_indices_ptr = debug_info['col_indices_ptr']\n",
    "row_indices_ptr = debug_info['row_indices_ptr']\n",
    "selected_experts = debug_info['selected_experts']\n",
    "selected_experts_sorted = debug_info['selected_experts_sorted']\n",
    "\n",
    "# Show some debug info\n",
    "print(f\"\\nRouter logits sample:\\n{router_logits[:3, :4]}\")\n",
    "print(f\"Block sparse sample:\\n{block_sparse[:5, :5]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Debug: Check which experts are being used\n",
    "print(\"=== EXPERT USAGE ANALYSIS ===\")\n",
    "print(\"col_indices_ptr:\", col_indices_ptr)\n",
    "\n",
    "# Use the debug info from the forward pass\n",
    "selected_experts_flat = selected_experts.reshape(-1)\n",
    "\n",
    "print(\"selected_experts_sorted (first 20):\", selected_experts_sorted[:20])\n",
    "print(\"selected_experts_flat (first 20):\", selected_experts_flat[:20])\n",
    "\n",
    "print(\"\\nExpert distribution:\")\n",
    "for i in range(config.num_experts):\n",
    "    count = (selected_experts_flat == i).sum()\n",
    "    print(f\"  Expert {i}: {count} blocks ({count/len(selected_experts_flat)*100:.1f}%)\")\n",
    "\n",
    "print(\"\\nRouter logits sample (first 5 tokens):\")\n",
    "print(router_logits[:5])\n",
    "\n",
    "print(\"\\nSelected experts (first 10 tokens):\")\n",
    "print(selected_experts[:10])\n",
    "\n",
    "# Get router weights for display\n",
    "router_weights = F.softmax(router_logits, dim=1, dtype=torch.float)\n",
    "router_weights_topk, _ = torch.topk(router_weights, config.num_experts_per_tok, dim=-1)\n",
    "if config.norm_topk_prob:\n",
    "    router_weights_topk /= router_weights_topk.sum(dim=-1, keepdim=True)\n",
    "\n",
    "print(\"\\nRouter weights (first 5 tokens):\")\n",
    "print(router_weights_topk[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Debug: Analyze block_sparse pattern\n",
    "print(\"=== BLOCK_SPARSE PATTERN ANALYSIS ===\")\n",
    "print(f\"block_sparse shape: {block_sparse.shape}\")\n",
    "\n",
    "# Check if certain columns are always zero\n",
    "nonzero_cols = torch.any(block_sparse != 0, dim=0)\n",
    "print(f\"Non-zero columns: {torch.sum(nonzero_cols)} out of {block_sparse.shape[1]}\")\n",
    "\n",
    "if torch.sum(nonzero_cols) > 0:\n",
    "    first_nonzero = torch.argmax(nonzero_cols.float())\n",
    "    last_nonzero = len(nonzero_cols) - 1 - torch.argmax(torch.flip(nonzero_cols.float(), [0]))\n",
    "    print(f\"First non-zero col: {first_nonzero}\")\n",
    "    print(f\"Last non-zero col: {last_nonzero}\")\n",
    "    print(f\"Non-zero range: columns {first_nonzero} to {last_nonzero}\")\n",
    "\n",
    "# Check weight matrix structure\n",
    "print(f\"\\nWeight matrix w1 shape: {moe_mlp.w1.shape}\")\n",
    "print(f\"d_ffn per expert: {moe_mlp.d_ffn}\")\n",
    "print(f\"Total experts: {config.num_experts}\")\n",
    "print(f\"Expected w1 width: {moe_mlp.d_ffn * config.num_experts}\")\n",
    "\n",
    "# Show which parts of w1 should correspond to each expert\n",
    "print(f\"\\nExpert weight matrix ranges:\")\n",
    "for i in range(config.num_experts):\n",
    "    start_col = i * moe_mlp.d_ffn\n",
    "    end_col = (i + 1) * moe_mlp.d_ffn\n",
    "    print(f\"  Expert {i}: columns {start_col} to {end_col-1}\")\n",
    "\n",
    "# Check basic stats\n",
    "print(f\"\\nblock_sparse stats:\")\n",
    "print(f\"  Non-zero elements: {torch.count_nonzero(block_sparse)}\")\n",
    "print(f\"  Min value: {block_sparse.min():.6f}\")\n",
    "print(f\"  Max value: {block_sparse.max():.6f}\")\n",
    "print(f\"  Mean: {block_sparse.mean():.6f}\")\n",
    "print(f\"  Std: {block_sparse.std():.6f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Debug: Check kernel indexing calculation\n",
    "print(\"=== KERNEL INDEXING DEBUG ===\")\n",
    "\n",
    "# Show the relationship between col_indices_ptr and the actual expert columns in w1\n",
    "print(f\"BLOCK_N: {moe_mlp.BLOCK_N}\")\n",
    "print(f\"col_indices_ptr: {col_indices_ptr}\")\n",
    "\n",
    "for i, expert_id in enumerate(col_indices_ptr):\n",
    "    expected_col_start = expert_id * moe_mlp.BLOCK_N  # This is what the kernel calculates\n",
    "    actual_expert_start = expert_id * moe_mlp.d_ffn   # This is where expert weights actually are\n",
    "    print(f\"Block {i}: expert_id={expert_id}\")\n",
    "    print(f\"  Kernel will use columns {expected_col_start} to {expected_col_start + moe_mlp.BLOCK_N - 1}\")\n",
    "    print(f\"  Expert {expert_id} weights are at columns {actual_expert_start} to {actual_expert_start + moe_mlp.d_ffn - 1}\")\n",
    "    print(f\"  Match: {expected_col_start == actual_expert_start and moe_mlp.BLOCK_N == moe_mlp.d_ffn}\")\n",
    "\n",
    "# Test with a simple manual calculation to verify the kernel is working\n",
    "print(f\"\\n=== MANUAL VERIFICATION ===\")\n",
    "print(f\"If all tokens use expert 0, we should see non-zeros in columns 0 to {moe_mlp.d_ffn-1}\")\n",
    "print(f\"If all tokens use expert 1, we should see non-zeros in columns {moe_mlp.d_ffn} to {2*moe_mlp.d_ffn-1}\")\n",
    "\n",
    "# Check the actual pattern\n",
    "active_regions = []\n",
    "for i in range(config.num_experts):\n",
    "    start_col = i * moe_mlp.d_ffn\n",
    "    end_col = (i + 1) * moe_mlp.d_ffn\n",
    "    region_has_values = torch.any(block_sparse[:, start_col:end_col] != 0)\n",
    "    active_regions.append(region_has_values.item())\n",
    "    print(f\"Expert {i} region (cols {start_col}-{end_col-1}): {'ACTIVE' if region_has_values else 'INACTIVE'}\")\n",
    "\n",
    "print(f\"\\nActive expert regions: {[i for i, active in enumerate(active_regions) if active]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the FULL block_sparse tensor (as requested)\n",
    "print(\"=== FULL BLOCK_SPARSE TENSOR ===\")\n",
    "torch.set_printoptions(threshold=float('inf'), linewidth=200)\n",
    "print(\"block_sparse (full tensor):\")\n",
    "print(block_sparse)\n",
    "\n",
    "# Reset print options to default\n",
    "torch.set_printoptions(profile=\"default\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
